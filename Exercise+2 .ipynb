{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2. Crime Dataset (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Exercise\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crim = pd.read_csv('crime_prep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>v_cont_0</th>\n",
       "      <th>v_cat_0</th>\n",
       "      <th>v_cat_1</th>\n",
       "      <th>v_cat_2</th>\n",
       "      <th>v_cat_3</th>\n",
       "      <th>v_cont_5</th>\n",
       "      <th>v_cont_6</th>\n",
       "      <th>v_cont_7</th>\n",
       "      <th>v_cont_8</th>\n",
       "      <th>...</th>\n",
       "      <th>v_cont_117</th>\n",
       "      <th>v_cont_118</th>\n",
       "      <th>v_cont_119</th>\n",
       "      <th>v_cont_120</th>\n",
       "      <th>v_cont_121</th>\n",
       "      <th>v_cont_122</th>\n",
       "      <th>v_cont_123</th>\n",
       "      <th>v_cont_124</th>\n",
       "      <th>v_cont_125</th>\n",
       "      <th>v_cont_126</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  v_cont_0  v_cat_0  v_cat_1       v_cat_2  v_cat_3  v_cont_5  \\\n",
       "0    0.20         8      NaN      NaN  Lakewoodcity        1      0.19   \n",
       "1    0.67        53      NaN      NaN   Tukwilacity        1      0.00   \n",
       "\n",
       "   v_cont_6  v_cont_7  v_cont_8     ...      v_cont_117  v_cont_118  \\\n",
       "0      0.33      0.02      0.90     ...            0.29        0.12   \n",
       "1      0.16      0.12      0.74     ...             NaN        0.02   \n",
       "\n",
       "   v_cont_119  v_cont_120  v_cont_121  v_cont_122  v_cont_123  v_cont_124  \\\n",
       "0        0.26        0.20        0.06        0.04         0.9         0.5   \n",
       "1        0.12        0.45         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   v_cont_125  v_cont_126  \n",
       "0        0.32        0.14  \n",
       "1        0.00         NaN  \n",
       "\n",
       "[2 rows x 128 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crim.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1994 entries, 0 to 1993\n",
      "Columns: 128 entries, target to v_cont_126\n",
      "dtypes: float64(125), int64(2), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "crim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>v_cont_0</th>\n",
       "      <th>v_cat_0</th>\n",
       "      <th>v_cat_1</th>\n",
       "      <th>v_cat_3</th>\n",
       "      <th>v_cont_5</th>\n",
       "      <th>v_cont_6</th>\n",
       "      <th>v_cont_7</th>\n",
       "      <th>v_cont_8</th>\n",
       "      <th>v_cont_9</th>\n",
       "      <th>...</th>\n",
       "      <th>v_cont_117</th>\n",
       "      <th>v_cont_118</th>\n",
       "      <th>v_cont_119</th>\n",
       "      <th>v_cont_120</th>\n",
       "      <th>v_cont_121</th>\n",
       "      <th>v_cont_122</th>\n",
       "      <th>v_cont_123</th>\n",
       "      <th>v_cont_124</th>\n",
       "      <th>v_cont_125</th>\n",
       "      <th>v_cont_126</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>820.000000</td>\n",
       "      <td>817.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.237979</td>\n",
       "      <td>28.683551</td>\n",
       "      <td>58.826829</td>\n",
       "      <td>46188.336597</td>\n",
       "      <td>5.493982</td>\n",
       "      <td>0.057593</td>\n",
       "      <td>0.463395</td>\n",
       "      <td>0.179629</td>\n",
       "      <td>0.753716</td>\n",
       "      <td>0.153681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305987</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>0.232854</td>\n",
       "      <td>0.161685</td>\n",
       "      <td>0.163103</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.698589</td>\n",
       "      <td>0.440439</td>\n",
       "      <td>0.094052</td>\n",
       "      <td>0.195078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.232985</td>\n",
       "      <td>16.397553</td>\n",
       "      <td>126.420560</td>\n",
       "      <td>25299.726569</td>\n",
       "      <td>2.873694</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.253442</td>\n",
       "      <td>0.244039</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226884</td>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.203092</td>\n",
       "      <td>0.229055</td>\n",
       "      <td>0.214778</td>\n",
       "      <td>0.140207</td>\n",
       "      <td>0.213944</td>\n",
       "      <td>0.405808</td>\n",
       "      <td>0.240328</td>\n",
       "      <td>0.164718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.070000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>25065.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.150000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>48090.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.330000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>66660.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>94597.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target     v_cont_0     v_cat_0       v_cat_1      v_cat_3  \\\n",
       "count  1994.000000  1994.000000  820.000000    817.000000  1994.000000   \n",
       "mean      0.237979    28.683551   58.826829  46188.336597     5.493982   \n",
       "std       0.232985    16.397553  126.420560  25299.726569     2.873694   \n",
       "min       0.000000     1.000000    1.000000     70.000000     1.000000   \n",
       "25%       0.070000    12.000000    9.000000  25065.000000     3.000000   \n",
       "50%       0.150000    34.000000   23.000000  48090.000000     5.000000   \n",
       "75%       0.330000    42.000000   59.500000  66660.000000     8.000000   \n",
       "max       1.000000    56.000000  840.000000  94597.000000    10.000000   \n",
       "\n",
       "          v_cont_5     v_cont_6     v_cont_7     v_cont_8     v_cont_9  \\\n",
       "count  1994.000000  1994.000000  1994.000000  1994.000000  1994.000000   \n",
       "mean      0.057593     0.463395     0.179629     0.753716     0.153681   \n",
       "std       0.126906     0.163717     0.253442     0.244039     0.208877   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.010000     0.350000     0.020000     0.630000     0.040000   \n",
       "50%       0.020000     0.440000     0.060000     0.850000     0.070000   \n",
       "75%       0.050000     0.540000     0.230000     0.940000     0.170000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          ...      v_cont_117   v_cont_118   v_cont_119   v_cont_120  \\\n",
       "count     ...      319.000000  1994.000000  1994.000000  1994.000000   \n",
       "mean      ...        0.305987     0.065231     0.232854     0.161685   \n",
       "std       ...        0.226884     0.109459     0.203092     0.229055   \n",
       "min       ...        0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...        0.140000     0.020000     0.100000     0.020000   \n",
       "50%       ...        0.260000     0.040000     0.170000     0.070000   \n",
       "75%       ...        0.395000     0.070000     0.280000     0.190000   \n",
       "max       ...        1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       v_cont_121  v_cont_122  v_cont_123  v_cont_124   v_cont_125  v_cont_126  \n",
       "count  319.000000  319.000000  319.000000  319.000000  1994.000000  319.000000  \n",
       "mean     0.163103    0.076708    0.698589    0.440439     0.094052    0.195078  \n",
       "std      0.214778    0.140207    0.213944    0.405808     0.240328    0.164718  \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.000000    0.000000  \n",
       "25%      0.040000    0.020000    0.620000    0.000000     0.000000    0.110000  \n",
       "50%      0.080000    0.030000    0.750000    0.500000     0.000000    0.150000  \n",
       "75%      0.195000    0.060000    0.840000    1.000000     0.000000    0.220000  \n",
       "max      1.000000    1.000000    1.000000    1.000000     1.000000    1.000000  \n",
       "\n",
       "[8 rows x 127 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crim.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     200\n",
       "3     200\n",
       "2     200\n",
       "1     200\n",
       "10    199\n",
       "9     199\n",
       "8     199\n",
       "7     199\n",
       "6     199\n",
       "5     199\n",
       "Name: v_cat_3, dtype: int64"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crim['v_cat_3'].value_counts()  # we have 10 different classes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Missing Data.\n",
    "We can use seaborn to create a simple heatmap to see where we are missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x144f500e2b0>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAAIECAYAAACe17IJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x144f500eb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(crim.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of missing data for v_cat_0,v_cat_1 and v_cont_101,v_cont_102,,...,v_cont_126,(v_cont_118,v_cont_119,v_cont_120,v_cont_125 are good) columns.It's better to drop them.Especially for the end of the data.Because the number of missing values is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crim.drop(['v_cat_0','v_cat_1','v_cont_101','v_cont_102','v_cont_103','v_cont_104','v_cont_105','v_cont_106','v_cont_107','v_cont_108','v_cont_109','v_cont_110','v_cont_111','v_cont_112','v_cont_113','v_cont_114','v_cont_115','v_cont_116','v_cont_117','v_cont_121','v_cont_122','v_cont_123','v_cont_124','v_cont_126'  ],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14484726160>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAAIECAYAAACe17IJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEOIAAAAAdOD/B0zifxc5ozyLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14484726f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we check one more time data set for missing data:\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(crim.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crim.dropna(inplace=True)\n",
    "y = crim['target']\n",
    "crim.drop('target',axis =1,inplace=True)\n",
    "X = crim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993,)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good to start modeling without any missing data.But befor that we need to do some preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two cathegorical variable(v_cat_2,v_cat_3) which need to be encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_cat2 = LabelEncoder()\n",
    "X.iloc[:, 1] = labelencoder_cat2.fit_transform(X.iloc[:, 1])\n",
    "labelencoder_cat3 = LabelEncoder()\n",
    "X.iloc[:, 2] = labelencoder_cat3.fit_transform(X.iloc[:,2])\n",
    "\n",
    "# we need to generate Dummy variable for these two variables\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [2])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Multiple Linear Regression to the Training set\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting the Test set results\n",
    "\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653737891539\n",
      "/n\n",
      "0.0496484879\n"
     ]
    }
   ],
   "source": [
    "#K-flod cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = regressor, X = X_train,y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print('/n')\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachward elimination and feature selection on the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building optimal model using the bachward elimination\n",
    "\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   39.26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:47:02</td>     <th>  Log-Likelihood:    </th> <td>  1270.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2316.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1881</td>      <th>  BIC:               </th> <td>  -1690.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   111</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.5132</td> <td>    0.184</td> <td>    2.782</td> <td> 0.005</td> <td>    0.151</td> <td>    0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0698</td> <td>    0.021</td> <td>    3.391</td> <td> 0.001</td> <td>    0.029</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0484</td> <td>    0.021</td> <td>    2.342</td> <td> 0.019</td> <td>    0.008</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0551</td> <td>    0.021</td> <td>    2.654</td> <td> 0.008</td> <td>    0.014</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0411</td> <td>    0.021</td> <td>    1.981</td> <td> 0.048</td> <td>    0.000</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0563</td> <td>    0.020</td> <td>    2.759</td> <td> 0.006</td> <td>    0.016</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0590</td> <td>    0.021</td> <td>    2.868</td> <td> 0.004</td> <td>    0.019</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.0449</td> <td>    0.021</td> <td>    2.180</td> <td> 0.029</td> <td>    0.004</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0473</td> <td>    0.021</td> <td>    2.305</td> <td> 0.021</td> <td>    0.007</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0524</td> <td>    0.020</td> <td>    2.568</td> <td> 0.010</td> <td>    0.012</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0389</td> <td>    0.020</td> <td>    1.901</td> <td> 0.057</td> <td>   -0.001</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0007</td> <td>    0.000</td> <td>   -2.818</td> <td> 0.005</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> -3.39e-06</td> <td> 5.81e-06</td> <td>   -0.584</td> <td> 0.560</td> <td>-1.48e-05</td> <td>    8e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.1635</td> <td>    0.399</td> <td>    0.409</td> <td> 0.682</td> <td>   -0.620</td> <td>    0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0083</td> <td>    0.087</td> <td>   -0.096</td> <td> 0.924</td> <td>   -0.178</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.2026</td> <td>    0.051</td> <td>    3.961</td> <td> 0.000</td> <td>    0.102</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.0512</td> <td>    0.059</td> <td>   -0.871</td> <td> 0.384</td> <td>   -0.166</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.0131</td> <td>    0.034</td> <td>   -0.382</td> <td> 0.703</td> <td>   -0.080</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.0553</td> <td>    0.053</td> <td>    1.035</td> <td> 0.301</td> <td>   -0.049</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.1230</td> <td>    0.106</td> <td>    1.160</td> <td> 0.246</td> <td>   -0.085</td> <td>    0.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.2271</td> <td>    0.157</td> <td>   -1.449</td> <td> 0.147</td> <td>   -0.534</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.1401</td> <td>    0.164</td> <td>   -0.852</td> <td> 0.394</td> <td>   -0.463</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.0414</td> <td>    0.104</td> <td>    0.399</td> <td> 0.690</td> <td>   -0.162</td> <td>    0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.2725</td> <td>    0.389</td> <td>   -0.701</td> <td> 0.483</td> <td>   -1.035</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0481</td> <td>    0.016</td> <td>    3.072</td> <td> 0.002</td> <td>    0.017</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.1725</td> <td>    0.173</td> <td>   -1.000</td> <td> 0.318</td> <td>   -0.511</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.1919</td> <td>    0.090</td> <td>   -2.139</td> <td> 0.033</td> <td>   -0.368</td> <td>   -0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0493</td> <td>    0.020</td> <td>    2.449</td> <td> 0.014</td> <td>    0.010</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.1569</td> <td>    0.068</td> <td>   -2.313</td> <td> 0.021</td> <td>   -0.290</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>    0.0898</td> <td>    0.107</td> <td>    0.836</td> <td> 0.403</td> <td>   -0.121</td> <td>    0.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.0059</td> <td>    0.046</td> <td>   -0.127</td> <td> 0.899</td> <td>   -0.096</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.0895</td> <td>    0.037</td> <td>   -2.426</td> <td> 0.015</td> <td>   -0.162</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.2709</td> <td>    0.160</td> <td>    1.690</td> <td> 0.091</td> <td>   -0.043</td> <td>    0.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.0930</td> <td>    0.189</td> <td>    0.493</td> <td> 0.622</td> <td>   -0.277</td> <td>    0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.3393</td> <td>    0.152</td> <td>   -2.227</td> <td> 0.026</td> <td>   -0.638</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>   -0.0319</td> <td>    0.025</td> <td>   -1.252</td> <td> 0.211</td> <td>   -0.082</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0341</td> <td>    0.019</td> <td>   -1.753</td> <td> 0.080</td> <td>   -0.072</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.0173</td> <td>    0.019</td> <td>    0.916</td> <td> 0.360</td> <td>   -0.020</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0443</td> <td>    0.019</td> <td>    2.365</td> <td> 0.018</td> <td>    0.008</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    0.0352</td> <td>    0.025</td> <td>    1.411</td> <td> 0.158</td> <td>   -0.014</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.1129</td> <td>    0.138</td> <td>    0.817</td> <td> 0.414</td> <td>   -0.158</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -0.1635</td> <td>    0.063</td> <td>   -2.602</td> <td> 0.009</td> <td>   -0.287</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.0999</td> <td>    0.068</td> <td>   -1.471</td> <td> 0.142</td> <td>   -0.233</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.0486</td> <td>    0.096</td> <td>    0.506</td> <td> 0.613</td> <td>   -0.140</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    0.0433</td> <td>    0.078</td> <td>    0.557</td> <td> 0.577</td> <td>   -0.109</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>    0.0041</td> <td>    0.041</td> <td>    0.101</td> <td> 0.920</td> <td>   -0.076</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>    0.2580</td> <td>    0.079</td> <td>    3.265</td> <td> 0.001</td> <td>    0.103</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.0615</td> <td>    0.032</td> <td>   -1.915</td> <td> 0.056</td> <td>   -0.125</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -0.0279</td> <td>    0.041</td> <td>   -0.683</td> <td> 0.495</td> <td>   -0.108</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.0700</td> <td>    0.055</td> <td>    1.272</td> <td> 0.203</td> <td>   -0.038</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>    0.1250</td> <td>    0.087</td> <td>    1.444</td> <td> 0.149</td> <td>   -0.045</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    0.4528</td> <td>    0.248</td> <td>    1.829</td> <td> 0.068</td> <td>   -0.033</td> <td>    0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.2192</td> <td>    0.068</td> <td>    3.224</td> <td> 0.001</td> <td>    0.086</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.1391</td> <td>    0.310</td> <td>    0.449</td> <td> 0.653</td> <td>   -0.468</td> <td>    0.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>   -0.5364</td> <td>    0.518</td> <td>   -1.035</td> <td> 0.301</td> <td>   -1.553</td> <td>    0.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>   -0.1654</td> <td>    0.168</td> <td>   -0.982</td> <td> 0.326</td> <td>   -0.496</td> <td>    0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>   -0.0151</td> <td>    0.160</td> <td>   -0.095</td> <td> 0.925</td> <td>   -0.328</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>   -0.2827</td> <td>    0.156</td> <td>   -1.815</td> <td> 0.070</td> <td>   -0.588</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>   -0.0258</td> <td>    0.048</td> <td>   -0.535</td> <td> 0.593</td> <td>   -0.120</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>   -0.0033</td> <td>    0.043</td> <td>   -0.077</td> <td> 0.939</td> <td>   -0.087</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.0582</td> <td>    0.047</td> <td>    1.236</td> <td> 0.217</td> <td>   -0.034</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>   -0.1927</td> <td>    0.054</td> <td>   -3.576</td> <td> 0.000</td> <td>   -0.298</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>   -0.1360</td> <td>    0.109</td> <td>   -1.251</td> <td> 0.211</td> <td>   -0.349</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>    0.1201</td> <td>    0.048</td> <td>    2.524</td> <td> 0.012</td> <td>    0.027</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>   -0.1414</td> <td>    0.078</td> <td>   -1.807</td> <td> 0.071</td> <td>   -0.295</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>    0.0235</td> <td>    0.041</td> <td>    0.574</td> <td> 0.566</td> <td>   -0.057</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>    0.0337</td> <td>    0.067</td> <td>    0.506</td> <td> 0.613</td> <td>   -0.097</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -0.0792</td> <td>    0.077</td> <td>   -1.026</td> <td> 0.305</td> <td>   -0.231</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>    0.0320</td> <td>    0.060</td> <td>    0.536</td> <td> 0.592</td> <td>   -0.085</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -0.0134</td> <td>    0.122</td> <td>   -0.109</td> <td> 0.913</td> <td>   -0.253</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>   -0.2356</td> <td>    0.222</td> <td>   -1.062</td> <td> 0.288</td> <td>   -0.671</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>    0.4092</td> <td>    0.274</td> <td>    1.495</td> <td> 0.135</td> <td>   -0.127</td> <td>    0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>   -0.1516</td> <td>    0.219</td> <td>   -0.692</td> <td> 0.489</td> <td>   -0.581</td> <td>    0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   -0.0184</td> <td>    0.070</td> <td>   -0.261</td> <td> 0.794</td> <td>   -0.157</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>   -0.1285</td> <td>    0.069</td> <td>   -1.873</td> <td> 0.061</td> <td>   -0.263</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>    0.0446</td> <td>    0.226</td> <td>    0.197</td> <td> 0.844</td> <td>   -0.399</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>   -0.1758</td> <td>    0.237</td> <td>   -0.743</td> <td> 0.458</td> <td>   -0.640</td> <td>    0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>    0.5821</td> <td>    0.251</td> <td>    2.323</td> <td> 0.020</td> <td>    0.091</td> <td>    1.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>   -0.0433</td> <td>    0.168</td> <td>   -0.258</td> <td> 0.797</td> <td>   -0.373</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>   -0.2432</td> <td>    0.081</td> <td>   -3.003</td> <td> 0.003</td> <td>   -0.402</td> <td>   -0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   -0.7079</td> <td>    0.359</td> <td>   -1.974</td> <td> 0.049</td> <td>   -1.411</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>    0.2072</td> <td>    0.076</td> <td>    2.737</td> <td> 0.006</td> <td>    0.059</td> <td>    0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>    0.0861</td> <td>    0.059</td> <td>    1.462</td> <td> 0.144</td> <td>   -0.029</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>    0.0290</td> <td>    0.020</td> <td>    1.488</td> <td> 0.137</td> <td>   -0.009</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>    0.1517</td> <td>    0.073</td> <td>    2.072</td> <td> 0.038</td> <td>    0.008</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>   -0.0495</td> <td>    0.031</td> <td>   -1.599</td> <td> 0.110</td> <td>   -0.110</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>    0.5725</td> <td>    0.375</td> <td>    1.526</td> <td> 0.127</td> <td>   -0.163</td> <td>    1.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>    0.0519</td> <td>    0.021</td> <td>    2.419</td> <td> 0.016</td> <td>    0.010</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>   -0.0731</td> <td>    0.025</td> <td>   -2.902</td> <td> 0.004</td> <td>   -0.122</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>   -0.0265</td> <td>    0.029</td> <td>   -0.915</td> <td> 0.360</td> <td>   -0.083</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>    0.0141</td> <td>    0.035</td> <td>    0.400</td> <td> 0.690</td> <td>   -0.055</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>   -0.0148</td> <td>    0.020</td> <td>   -0.729</td> <td> 0.466</td> <td>   -0.054</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>   -0.3941</td> <td>    0.205</td> <td>   -1.924</td> <td> 0.055</td> <td>   -0.796</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>    0.2641</td> <td>    0.307</td> <td>    0.859</td> <td> 0.390</td> <td>   -0.339</td> <td>    0.867</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>    0.0220</td> <td>    0.165</td> <td>    0.133</td> <td> 0.894</td> <td>   -0.301</td> <td>    0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>   -0.2246</td> <td>    0.067</td> <td>   -3.348</td> <td> 0.001</td> <td>   -0.356</td> <td>   -0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>   -0.0181</td> <td>    0.157</td> <td>   -0.116</td> <td> 0.908</td> <td>   -0.326</td> <td>    0.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>   -0.0536</td> <td>    0.086</td> <td>   -0.621</td> <td> 0.535</td> <td>   -0.223</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>    0.3510</td> <td>    0.130</td> <td>    2.702</td> <td> 0.007</td> <td>    0.096</td> <td>    0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>    0.0408</td> <td>    0.033</td> <td>    1.253</td> <td> 0.211</td> <td>   -0.023</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>   -0.0403</td> <td>    0.035</td> <td>   -1.169</td> <td> 0.243</td> <td>   -0.108</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>   -0.0708</td> <td>    0.025</td> <td>   -2.871</td> <td> 0.004</td> <td>   -0.119</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td>    0.1466</td> <td>    0.064</td> <td>    2.279</td> <td> 0.023</td> <td>    0.020</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td>    0.1739</td> <td>    0.047</td> <td>    3.680</td> <td> 0.000</td> <td>    0.081</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td>    0.1096</td> <td>    0.090</td> <td>    1.217</td> <td> 0.224</td> <td>   -0.067</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td>    0.0163</td> <td>    0.042</td> <td>    0.389</td> <td> 0.697</td> <td>   -0.066</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td>   -0.0039</td> <td>    0.058</td> <td>   -0.067</td> <td> 0.946</td> <td>   -0.117</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>    0.0193</td> <td>    0.038</td> <td>    0.505</td> <td> 0.613</td> <td>   -0.056</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td>    0.0154</td> <td>    0.043</td> <td>    0.362</td> <td> 0.718</td> <td>   -0.068</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td>    0.0190</td> <td>    0.049</td> <td>    0.386</td> <td> 0.700</td> <td>   -0.077</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td>   -0.0106</td> <td>    0.030</td> <td>   -0.350</td> <td> 0.727</td> <td>   -0.070</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td>   -0.0386</td> <td>    0.023</td> <td>   -1.667</td> <td> 0.096</td> <td>   -0.084</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td>    0.0231</td> <td>    0.015</td> <td>    1.496</td> <td> 0.135</td> <td>   -0.007</td> <td>    0.053</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>375.293</td> <th>  Durbin-Watson:     </th> <td>   2.010</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1200.889</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.937</td>  <th>  Prob(JB):          </th> <td>1.70e-261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.309</td>  <th>  Cond. No.          </th> <td>2.76e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.699\n",
       "Model:                            OLS   Adj. R-squared:                  0.681\n",
       "Method:                 Least Squares   F-statistic:                     39.26\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        13:47:02   Log-Likelihood:                 1270.2\n",
       "No. Observations:                1993   AIC:                            -2316.\n",
       "Df Residuals:                    1881   BIC:                            -1690.\n",
       "Df Model:                         111                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.5132      0.184      2.782      0.005       0.151       0.875\n",
       "x1             0.0698      0.021      3.391      0.001       0.029       0.110\n",
       "x2             0.0484      0.021      2.342      0.019       0.008       0.089\n",
       "x3             0.0551      0.021      2.654      0.008       0.014       0.096\n",
       "x4             0.0411      0.021      1.981      0.048       0.000       0.082\n",
       "x5             0.0563      0.020      2.759      0.006       0.016       0.096\n",
       "x6             0.0590      0.021      2.868      0.004       0.019       0.099\n",
       "x7             0.0449      0.021      2.180      0.029       0.004       0.085\n",
       "x8             0.0473      0.021      2.305      0.021       0.007       0.088\n",
       "x9             0.0524      0.020      2.568      0.010       0.012       0.092\n",
       "x10            0.0389      0.020      1.901      0.057      -0.001       0.079\n",
       "x11           -0.0007      0.000     -2.818      0.005      -0.001      -0.000\n",
       "x12         -3.39e-06   5.81e-06     -0.584      0.560   -1.48e-05       8e-06\n",
       "x13            0.1635      0.399      0.409      0.682      -0.620       0.947\n",
       "x14           -0.0083      0.087     -0.096      0.924      -0.178       0.162\n",
       "x15            0.2026      0.051      3.961      0.000       0.102       0.303\n",
       "x16           -0.0512      0.059     -0.871      0.384      -0.166       0.064\n",
       "x17           -0.0131      0.034     -0.382      0.703      -0.080       0.054\n",
       "x18            0.0553      0.053      1.035      0.301      -0.049       0.160\n",
       "x19            0.1230      0.106      1.160      0.246      -0.085       0.331\n",
       "x20           -0.2271      0.157     -1.449      0.147      -0.534       0.080\n",
       "x21           -0.1401      0.164     -0.852      0.394      -0.463       0.182\n",
       "x22            0.0414      0.104      0.399      0.690      -0.162       0.245\n",
       "x23           -0.2725      0.389     -0.701      0.483      -1.035       0.490\n",
       "x24            0.0481      0.016      3.072      0.002       0.017       0.079\n",
       "x25           -0.1725      0.173     -1.000      0.318      -0.511       0.166\n",
       "x26           -0.1919      0.090     -2.139      0.033      -0.368      -0.016\n",
       "x27            0.0493      0.020      2.449      0.014       0.010       0.089\n",
       "x28           -0.1569      0.068     -2.313      0.021      -0.290      -0.024\n",
       "x29            0.0898      0.107      0.836      0.403      -0.121       0.300\n",
       "x30           -0.0059      0.046     -0.127      0.899      -0.096       0.085\n",
       "x31           -0.0895      0.037     -2.426      0.015      -0.162      -0.017\n",
       "x32            0.2709      0.160      1.690      0.091      -0.043       0.585\n",
       "x33            0.0930      0.189      0.493      0.622      -0.277       0.463\n",
       "x34           -0.3393      0.152     -2.227      0.026      -0.638      -0.040\n",
       "x35           -0.0319      0.025     -1.252      0.211      -0.082       0.018\n",
       "x36           -0.0341      0.019     -1.753      0.080      -0.072       0.004\n",
       "x37            0.0173      0.019      0.916      0.360      -0.020       0.054\n",
       "x38            0.0443      0.019      2.365      0.018       0.008       0.081\n",
       "x39            0.0352      0.025      1.411      0.158      -0.014       0.084\n",
       "x40            0.1129      0.138      0.817      0.414      -0.158       0.384\n",
       "x41           -0.1635      0.063     -2.602      0.009      -0.287      -0.040\n",
       "x42           -0.0999      0.068     -1.471      0.142      -0.233       0.033\n",
       "x43            0.0486      0.096      0.506      0.613      -0.140       0.237\n",
       "x44            0.0433      0.078      0.557      0.577      -0.109       0.196\n",
       "x45            0.0041      0.041      0.101      0.920      -0.076       0.084\n",
       "x46            0.2580      0.079      3.265      0.001       0.103       0.413\n",
       "x47           -0.0615      0.032     -1.915      0.056      -0.125       0.001\n",
       "x48           -0.0279      0.041     -0.683      0.495      -0.108       0.052\n",
       "x49            0.0700      0.055      1.272      0.203      -0.038       0.178\n",
       "x50            0.1250      0.087      1.444      0.149      -0.045       0.295\n",
       "x51            0.4528      0.248      1.829      0.068      -0.033       0.938\n",
       "x52            0.2192      0.068      3.224      0.001       0.086       0.352\n",
       "x53            0.1391      0.310      0.449      0.653      -0.468       0.746\n",
       "x54           -0.5364      0.518     -1.035      0.301      -1.553       0.480\n",
       "x55           -0.1654      0.168     -0.982      0.326      -0.496       0.165\n",
       "x56           -0.0151      0.160     -0.095      0.925      -0.328       0.298\n",
       "x57           -0.2827      0.156     -1.815      0.070      -0.588       0.023\n",
       "x58           -0.0258      0.048     -0.535      0.593      -0.120       0.069\n",
       "x59           -0.0033      0.043     -0.077      0.939      -0.087       0.080\n",
       "x60            0.0582      0.047      1.236      0.217      -0.034       0.151\n",
       "x61           -0.1927      0.054     -3.576      0.000      -0.298      -0.087\n",
       "x62           -0.1360      0.109     -1.251      0.211      -0.349       0.077\n",
       "x63            0.1201      0.048      2.524      0.012       0.027       0.213\n",
       "x64           -0.1414      0.078     -1.807      0.071      -0.295       0.012\n",
       "x65            0.0235      0.041      0.574      0.566      -0.057       0.104\n",
       "x66            0.0337      0.067      0.506      0.613      -0.097       0.164\n",
       "x67           -0.0792      0.077     -1.026      0.305      -0.231       0.072\n",
       "x68            0.0320      0.060      0.536      0.592      -0.085       0.149\n",
       "x69           -0.0134      0.122     -0.109      0.913      -0.253       0.226\n",
       "x70           -0.2356      0.222     -1.062      0.288      -0.671       0.200\n",
       "x71            0.4092      0.274      1.495      0.135      -0.127       0.946\n",
       "x72           -0.1516      0.219     -0.692      0.489      -0.581       0.278\n",
       "x73           -0.0184      0.070     -0.261      0.794      -0.157       0.120\n",
       "x74           -0.1285      0.069     -1.873      0.061      -0.263       0.006\n",
       "x75            0.0446      0.226      0.197      0.844      -0.399       0.488\n",
       "x76           -0.1758      0.237     -0.743      0.458      -0.640       0.288\n",
       "x77            0.5821      0.251      2.323      0.020       0.091       1.073\n",
       "x78           -0.0433      0.168     -0.258      0.797      -0.373       0.286\n",
       "x79           -0.2432      0.081     -3.003      0.003      -0.402      -0.084\n",
       "x80           -0.7079      0.359     -1.974      0.049      -1.411      -0.005\n",
       "x81            0.2072      0.076      2.737      0.006       0.059       0.356\n",
       "x82            0.0861      0.059      1.462      0.144      -0.029       0.202\n",
       "x83            0.0290      0.020      1.488      0.137      -0.009       0.067\n",
       "x84            0.1517      0.073      2.072      0.038       0.008       0.295\n",
       "x85           -0.0495      0.031     -1.599      0.110      -0.110       0.011\n",
       "x86            0.5725      0.375      1.526      0.127      -0.163       1.308\n",
       "x87            0.0519      0.021      2.419      0.016       0.010       0.094\n",
       "x88           -0.0731      0.025     -2.902      0.004      -0.122      -0.024\n",
       "x89           -0.0265      0.029     -0.915      0.360      -0.083       0.030\n",
       "x90            0.0141      0.035      0.400      0.690      -0.055       0.084\n",
       "x91           -0.0148      0.020     -0.729      0.466      -0.054       0.025\n",
       "x92           -0.3941      0.205     -1.924      0.055      -0.796       0.008\n",
       "x93            0.2641      0.307      0.859      0.390      -0.339       0.867\n",
       "x94            0.0220      0.165      0.133      0.894      -0.301       0.345\n",
       "x95           -0.2246      0.067     -3.348      0.001      -0.356      -0.093\n",
       "x96           -0.0181      0.157     -0.116      0.908      -0.326       0.289\n",
       "x97           -0.0536      0.086     -0.621      0.535      -0.223       0.116\n",
       "x98            0.3510      0.130      2.702      0.007       0.096       0.606\n",
       "x99            0.0408      0.033      1.253      0.211      -0.023       0.105\n",
       "x100          -0.0403      0.035     -1.169      0.243      -0.108       0.027\n",
       "x101          -0.0708      0.025     -2.871      0.004      -0.119      -0.022\n",
       "x102           0.1466      0.064      2.279      0.023       0.020       0.273\n",
       "x103           0.1739      0.047      3.680      0.000       0.081       0.267\n",
       "x104           0.1096      0.090      1.217      0.224      -0.067       0.286\n",
       "x105           0.0163      0.042      0.389      0.697      -0.066       0.098\n",
       "x106          -0.0039      0.058     -0.067      0.946      -0.117       0.109\n",
       "x107           0.0193      0.038      0.505      0.613      -0.056       0.094\n",
       "x108           0.0154      0.043      0.362      0.718      -0.068       0.099\n",
       "x109           0.0190      0.049      0.386      0.700      -0.077       0.115\n",
       "x110          -0.0106      0.030     -0.350      0.727      -0.070       0.049\n",
       "x111          -0.0386      0.023     -1.667      0.096      -0.084       0.007\n",
       "x112           0.0231      0.015      1.496      0.135      -0.007       0.053\n",
       "==============================================================================\n",
       "Omnibus:                      375.293   Durbin-Watson:                   2.010\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1200.889\n",
       "Skew:                           0.937   Prob(JB):                    1.70e-261\n",
       "Kurtosis:                       6.309   Cond. No.                     2.76e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.84e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate 1\n",
    "\n",
    "# adding column one for b0, the first coefficient of regression equation.\n",
    "X= np.append(arr = np.ones((1993,1)).astype(int),values =X , axis = 1)\n",
    "X_opt = X[:,:]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993, 113)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this summary we have p-value for all of our variables .So we know that the variables that have p-value more close to zero,they have more positive correlation with Target value.For example the variables : x15 , x103 , x95 ,x79 , x52 , x46 , x41 ,  x24 , x6 x5 , x1 are  highly correlated with the target. So we try to eliminate the variables that have maximum p-value at each iteration of backward elimination process by looking at the significance level that we selected already (for example SL= 0.05).we can eliminate a set of this variable at each iteration as well.At each step by eliminating the variables we should have better Adj R-squared value.Once Adj R- square decrease or all of p-values are less than SL, we will stop the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   42.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:47:51</td>     <th>  Log-Likelihood:    </th> <td>  1270.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2332.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1889</td>      <th>  BIC:               </th> <td>  -1750.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   103</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>   <td>    0.5162</td> <td>    0.182</td> <td>    2.831</td> <td> 0.005</td> <td>    0.159</td> <td>    0.874</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>   <td>    0.0701</td> <td>    0.020</td> <td>    3.439</td> <td> 0.001</td> <td>    0.030</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>   <td>    0.0485</td> <td>    0.020</td> <td>    2.376</td> <td> 0.018</td> <td>    0.008</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>   <td>    0.0555</td> <td>    0.021</td> <td>    2.696</td> <td> 0.007</td> <td>    0.015</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>    0.0414</td> <td>    0.021</td> <td>    2.018</td> <td> 0.044</td> <td>    0.001</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.0566</td> <td>    0.020</td> <td>    2.804</td> <td> 0.005</td> <td>    0.017</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0594</td> <td>    0.020</td> <td>    2.914</td> <td> 0.004</td> <td>    0.019</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.0451</td> <td>    0.020</td> <td>    2.207</td> <td> 0.027</td> <td>    0.005</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0476</td> <td>    0.020</td> <td>    2.345</td> <td> 0.019</td> <td>    0.008</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0526</td> <td>    0.020</td> <td>    2.610</td> <td> 0.009</td> <td>    0.013</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0393</td> <td>    0.020</td> <td>    1.938</td> <td> 0.053</td> <td>   -0.000</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>   -0.0007</td> <td>    0.000</td> <td>   -2.866</td> <td> 0.004</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>  <td>-3.363e-06</td> <td> 5.79e-06</td> <td>   -0.581</td> <td> 0.561</td> <td>-1.47e-05</td> <td> 7.99e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>  <td>    0.1595</td> <td>    0.397</td> <td>    0.401</td> <td> 0.688</td> <td>   -0.620</td> <td>    0.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>    0.2023</td> <td>    0.050</td> <td>    4.016</td> <td> 0.000</td> <td>    0.103</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>  <td>   -0.0500</td> <td>    0.058</td> <td>   -0.861</td> <td> 0.389</td> <td>   -0.164</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>  <td>   -0.0141</td> <td>    0.034</td> <td>   -0.417</td> <td> 0.677</td> <td>   -0.080</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>    0.0543</td> <td>    0.053</td> <td>    1.029</td> <td> 0.303</td> <td>   -0.049</td> <td>    0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>  <td>    0.1241</td> <td>    0.103</td> <td>    1.206</td> <td> 0.228</td> <td>   -0.078</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.2276</td> <td>    0.155</td> <td>   -1.469</td> <td> 0.142</td> <td>   -0.531</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>  <td>   -0.1428</td> <td>    0.162</td> <td>   -0.880</td> <td> 0.379</td> <td>   -0.461</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>  <td>    0.0410</td> <td>    0.102</td> <td>    0.402</td> <td> 0.688</td> <td>   -0.159</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>  <td>   -0.2679</td> <td>    0.387</td> <td>   -0.693</td> <td> 0.488</td> <td>   -1.026</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.0479</td> <td>    0.016</td> <td>    3.077</td> <td> 0.002</td> <td>    0.017</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>  <td>   -0.1698</td> <td>    0.170</td> <td>   -0.996</td> <td> 0.319</td> <td>   -0.504</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.1924</td> <td>    0.088</td> <td>   -2.177</td> <td> 0.030</td> <td>   -0.366</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>    0.0492</td> <td>    0.020</td> <td>    2.466</td> <td> 0.014</td> <td>    0.010</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.1569</td> <td>    0.067</td> <td>   -2.344</td> <td> 0.019</td> <td>   -0.288</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.0885</td> <td>    0.106</td> <td>    0.838</td> <td> 0.402</td> <td>   -0.119</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th>  <td>   -0.0048</td> <td>    0.044</td> <td>   -0.111</td> <td> 0.912</td> <td>   -0.091</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>   -0.0889</td> <td>    0.036</td> <td>   -2.484</td> <td> 0.013</td> <td>   -0.159</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.2673</td> <td>    0.158</td> <td>    1.690</td> <td> 0.091</td> <td>   -0.043</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th>  <td>    0.0935</td> <td>    0.185</td> <td>    0.505</td> <td> 0.614</td> <td>   -0.270</td> <td>    0.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.3375</td> <td>    0.151</td> <td>   -2.243</td> <td> 0.025</td> <td>   -0.633</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>  <td>   -0.0311</td> <td>    0.025</td> <td>   -1.232</td> <td> 0.218</td> <td>   -0.081</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>   -0.0340</td> <td>    0.019</td> <td>   -1.761</td> <td> 0.078</td> <td>   -0.072</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>  <td>    0.0174</td> <td>    0.019</td> <td>    0.921</td> <td> 0.357</td> <td>   -0.020</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.0444</td> <td>    0.019</td> <td>    2.379</td> <td> 0.017</td> <td>    0.008</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>    0.0351</td> <td>    0.025</td> <td>    1.418</td> <td> 0.156</td> <td>   -0.013</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>  <td>    0.1112</td> <td>    0.138</td> <td>    0.808</td> <td> 0.419</td> <td>   -0.159</td> <td>    0.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>   -0.1605</td> <td>    0.061</td> <td>   -2.627</td> <td> 0.009</td> <td>   -0.280</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>   -0.1018</td> <td>    0.067</td> <td>   -1.519</td> <td> 0.129</td> <td>   -0.233</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>43</th>  <td>    0.0466</td> <td>    0.094</td> <td>    0.493</td> <td> 0.622</td> <td>   -0.139</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>44</th>  <td>    0.0426</td> <td>    0.077</td> <td>    0.554</td> <td> 0.580</td> <td>   -0.108</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>    0.2584</td> <td>    0.066</td> <td>    3.896</td> <td> 0.000</td> <td>    0.128</td> <td>    0.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0612</td> <td>    0.032</td> <td>   -1.915</td> <td> 0.056</td> <td>   -0.124</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>48</th>  <td>   -0.0276</td> <td>    0.041</td> <td>   -0.680</td> <td> 0.496</td> <td>   -0.107</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>    0.0700</td> <td>    0.055</td> <td>    1.282</td> <td> 0.200</td> <td>   -0.037</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.1231</td> <td>    0.085</td> <td>    1.440</td> <td> 0.150</td> <td>   -0.045</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.4492</td> <td>    0.245</td> <td>    1.836</td> <td> 0.067</td> <td>   -0.031</td> <td>    0.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.2184</td> <td>    0.065</td> <td>    3.360</td> <td> 0.001</td> <td>    0.091</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>53</th>  <td>    0.1373</td> <td>    0.308</td> <td>    0.446</td> <td> 0.655</td> <td>   -0.466</td> <td>    0.741</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.5303</td> <td>    0.513</td> <td>   -1.033</td> <td> 0.302</td> <td>   -1.537</td> <td>    0.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>   -0.1451</td> <td>    0.142</td> <td>   -1.019</td> <td> 0.308</td> <td>   -0.424</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.2999</td> <td>    0.092</td> <td>   -3.275</td> <td> 0.001</td> <td>   -0.479</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>58</th>  <td>   -0.0271</td> <td>    0.046</td> <td>   -0.584</td> <td> 0.559</td> <td>   -0.118</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>  <td>    0.0581</td> <td>    0.047</td> <td>    1.242</td> <td> 0.214</td> <td>   -0.034</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.1923</td> <td>    0.053</td> <td>   -3.614</td> <td> 0.000</td> <td>   -0.297</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.1346</td> <td>    0.108</td> <td>   -1.247</td> <td> 0.212</td> <td>   -0.346</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.1208</td> <td>    0.047</td> <td>    2.578</td> <td> 0.010</td> <td>    0.029</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.1411</td> <td>    0.078</td> <td>   -1.813</td> <td> 0.070</td> <td>   -0.294</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>  <td>    0.0208</td> <td>    0.033</td> <td>    0.632</td> <td> 0.527</td> <td>   -0.044</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>66</th>  <td>    0.0366</td> <td>    0.061</td> <td>    0.601</td> <td> 0.548</td> <td>   -0.083</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>  <td>   -0.0795</td> <td>    0.077</td> <td>   -1.033</td> <td> 0.302</td> <td>   -0.230</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>  <td>    0.0313</td> <td>    0.059</td> <td>    0.527</td> <td> 0.598</td> <td>   -0.085</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>   -0.2509</td> <td>    0.159</td> <td>   -1.580</td> <td> 0.114</td> <td>   -0.562</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>    0.4107</td> <td>    0.272</td> <td>    1.508</td> <td> 0.132</td> <td>   -0.123</td> <td>    0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>  <td>   -0.1473</td> <td>    0.215</td> <td>   -0.684</td> <td> 0.494</td> <td>   -0.570</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>73</th>  <td>   -0.0194</td> <td>    0.070</td> <td>   -0.278</td> <td> 0.781</td> <td>   -0.156</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>   -0.1274</td> <td>    0.068</td> <td>   -1.868</td> <td> 0.062</td> <td>   -0.261</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>   -0.1316</td> <td>    0.077</td> <td>   -1.711</td> <td> 0.087</td> <td>   -0.282</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>    0.5507</td> <td>    0.212</td> <td>    2.599</td> <td> 0.009</td> <td>    0.135</td> <td>    0.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>78</th>  <td>   -0.0413</td> <td>    0.167</td> <td>   -0.248</td> <td> 0.804</td> <td>   -0.368</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>   -0.2434</td> <td>    0.079</td> <td>   -3.064</td> <td> 0.002</td> <td>   -0.399</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.7197</td> <td>    0.355</td> <td>   -2.028</td> <td> 0.043</td> <td>   -1.416</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>    0.2077</td> <td>    0.074</td> <td>    2.794</td> <td> 0.005</td> <td>    0.062</td> <td>    0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>    0.0852</td> <td>    0.058</td> <td>    1.466</td> <td> 0.143</td> <td>   -0.029</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>    0.0286</td> <td>    0.019</td> <td>    1.478</td> <td> 0.140</td> <td>   -0.009</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>    0.1519</td> <td>    0.073</td> <td>    2.084</td> <td> 0.037</td> <td>    0.009</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>   -0.0499</td> <td>    0.031</td> <td>   -1.627</td> <td> 0.104</td> <td>   -0.110</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>    0.5822</td> <td>    0.372</td> <td>    1.567</td> <td> 0.117</td> <td>   -0.147</td> <td>    1.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>    0.0521</td> <td>    0.021</td> <td>    2.450</td> <td> 0.014</td> <td>    0.010</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0732</td> <td>    0.025</td> <td>   -2.936</td> <td> 0.003</td> <td>   -0.122</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>  <td>   -0.0257</td> <td>    0.028</td> <td>   -0.925</td> <td> 0.355</td> <td>   -0.080</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>90</th>  <td>    0.0134</td> <td>    0.035</td> <td>    0.382</td> <td> 0.702</td> <td>   -0.055</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>  <td>   -0.0147</td> <td>    0.020</td> <td>   -0.729</td> <td> 0.466</td> <td>   -0.054</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.3968</td> <td>    0.201</td> <td>   -1.970</td> <td> 0.049</td> <td>   -0.792</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>    0.2652</td> <td>    0.305</td> <td>    0.871</td> <td> 0.384</td> <td>   -0.332</td> <td>    0.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>94</th>  <td>    0.0213</td> <td>    0.163</td> <td>    0.131</td> <td> 0.896</td> <td>   -0.298</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>   -0.2280</td> <td>    0.058</td> <td>   -3.906</td> <td> 0.000</td> <td>   -0.342</td> <td>   -0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>   -0.0576</td> <td>    0.082</td> <td>   -0.706</td> <td> 0.480</td> <td>   -0.218</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>    0.3427</td> <td>    0.100</td> <td>    3.441</td> <td> 0.001</td> <td>    0.147</td> <td>    0.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>  <td>    0.0413</td> <td>    0.032</td> <td>    1.281</td> <td> 0.200</td> <td>   -0.022</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0388</td> <td>    0.033</td> <td>   -1.175</td> <td> 0.240</td> <td>   -0.104</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>   -0.0709</td> <td>    0.024</td> <td>   -2.930</td> <td> 0.003</td> <td>   -0.118</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>    0.1472</td> <td>    0.064</td> <td>    2.301</td> <td> 0.021</td> <td>    0.022</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.1743</td> <td>    0.047</td> <td>    3.703</td> <td> 0.000</td> <td>    0.082</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.1062</td> <td>    0.089</td> <td>    1.199</td> <td> 0.231</td> <td>   -0.067</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>105</th> <td>    0.0152</td> <td>    0.041</td> <td>    0.371</td> <td> 0.710</td> <td>   -0.065</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th> <td>    0.0186</td> <td>    0.035</td> <td>    0.533</td> <td> 0.594</td> <td>   -0.050</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>108</th> <td>    0.0164</td> <td>    0.041</td> <td>    0.400</td> <td> 0.689</td> <td>   -0.064</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>109</th> <td>    0.0182</td> <td>    0.049</td> <td>    0.372</td> <td> 0.710</td> <td>   -0.078</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>110</th> <td>   -0.0103</td> <td>    0.030</td> <td>   -0.343</td> <td> 0.732</td> <td>   -0.070</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>   -0.0384</td> <td>    0.023</td> <td>   -1.693</td> <td> 0.091</td> <td>   -0.083</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.0234</td> <td>    0.015</td> <td>    1.519</td> <td> 0.129</td> <td>   -0.007</td> <td>    0.054</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>374.385</td> <th>  Durbin-Watson:     </th> <td>   2.010</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1195.488</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.936</td>  <th>  Prob(JB):          </th> <td>2.53e-260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.301</td>  <th>  Cond. No.          </th> <td>2.24e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.698\n",
       "Model:                            OLS   Adj. R-squared:                  0.682\n",
       "Method:                 Least Squares   F-statistic:                     42.49\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        13:47:51   Log-Likelihood:                 1270.2\n",
       "No. Observations:                1993   AIC:                            -2332.\n",
       "Df Residuals:                    1889   BIC:                            -1750.\n",
       "Df Model:                         103                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.5162      0.182      2.831      0.005       0.159       0.874\n",
       "1              0.0701      0.020      3.439      0.001       0.030       0.110\n",
       "2              0.0485      0.020      2.376      0.018       0.008       0.089\n",
       "3              0.0555      0.021      2.696      0.007       0.015       0.096\n",
       "4              0.0414      0.021      2.018      0.044       0.001       0.082\n",
       "5              0.0566      0.020      2.804      0.005       0.017       0.096\n",
       "6              0.0594      0.020      2.914      0.004       0.019       0.099\n",
       "7              0.0451      0.020      2.207      0.027       0.005       0.085\n",
       "8              0.0476      0.020      2.345      0.019       0.008       0.087\n",
       "9              0.0526      0.020      2.610      0.009       0.013       0.092\n",
       "10             0.0393      0.020      1.938      0.053      -0.000       0.079\n",
       "11            -0.0007      0.000     -2.866      0.004      -0.001      -0.000\n",
       "12         -3.363e-06   5.79e-06     -0.581      0.561   -1.47e-05    7.99e-06\n",
       "13             0.1595      0.397      0.401      0.688      -0.620       0.939\n",
       "15             0.2023      0.050      4.016      0.000       0.103       0.301\n",
       "16            -0.0500      0.058     -0.861      0.389      -0.164       0.064\n",
       "17            -0.0141      0.034     -0.417      0.677      -0.080       0.052\n",
       "18             0.0543      0.053      1.029      0.303      -0.049       0.158\n",
       "19             0.1241      0.103      1.206      0.228      -0.078       0.326\n",
       "20            -0.2276      0.155     -1.469      0.142      -0.531       0.076\n",
       "21            -0.1428      0.162     -0.880      0.379      -0.461       0.176\n",
       "22             0.0410      0.102      0.402      0.688      -0.159       0.241\n",
       "23            -0.2679      0.387     -0.693      0.488      -1.026       0.490\n",
       "24             0.0479      0.016      3.077      0.002       0.017       0.078\n",
       "25            -0.1698      0.170     -0.996      0.319      -0.504       0.164\n",
       "26            -0.1924      0.088     -2.177      0.030      -0.366      -0.019\n",
       "27             0.0492      0.020      2.466      0.014       0.010       0.088\n",
       "28            -0.1569      0.067     -2.344      0.019      -0.288      -0.026\n",
       "29             0.0885      0.106      0.838      0.402      -0.119       0.296\n",
       "30            -0.0048      0.044     -0.111      0.912      -0.091       0.081\n",
       "31            -0.0889      0.036     -2.484      0.013      -0.159      -0.019\n",
       "32             0.2673      0.158      1.690      0.091      -0.043       0.577\n",
       "33             0.0935      0.185      0.505      0.614      -0.270       0.457\n",
       "34            -0.3375      0.151     -2.243      0.025      -0.633      -0.042\n",
       "35            -0.0311      0.025     -1.232      0.218      -0.081       0.018\n",
       "36            -0.0340      0.019     -1.761      0.078      -0.072       0.004\n",
       "37             0.0174      0.019      0.921      0.357      -0.020       0.054\n",
       "38             0.0444      0.019      2.379      0.017       0.008       0.081\n",
       "39             0.0351      0.025      1.418      0.156      -0.013       0.084\n",
       "40             0.1112      0.138      0.808      0.419      -0.159       0.381\n",
       "41            -0.1605      0.061     -2.627      0.009      -0.280      -0.041\n",
       "42            -0.1018      0.067     -1.519      0.129      -0.233       0.030\n",
       "43             0.0466      0.094      0.493      0.622      -0.139       0.232\n",
       "44             0.0426      0.077      0.554      0.580      -0.108       0.194\n",
       "46             0.2584      0.066      3.896      0.000       0.128       0.388\n",
       "47            -0.0612      0.032     -1.915      0.056      -0.124       0.001\n",
       "48            -0.0276      0.041     -0.680      0.496      -0.107       0.052\n",
       "49             0.0700      0.055      1.282      0.200      -0.037       0.177\n",
       "50             0.1231      0.085      1.440      0.150      -0.045       0.291\n",
       "51             0.4492      0.245      1.836      0.067      -0.031       0.929\n",
       "52             0.2184      0.065      3.360      0.001       0.091       0.346\n",
       "53             0.1373      0.308      0.446      0.655      -0.466       0.741\n",
       "54            -0.5303      0.513     -1.033      0.302      -1.537       0.476\n",
       "55            -0.1451      0.142     -1.019      0.308      -0.424       0.134\n",
       "57            -0.2999      0.092     -3.275      0.001      -0.479      -0.120\n",
       "58            -0.0271      0.046     -0.584      0.559      -0.118       0.064\n",
       "60             0.0581      0.047      1.242      0.214      -0.034       0.150\n",
       "61            -0.1923      0.053     -3.614      0.000      -0.297      -0.088\n",
       "62            -0.1346      0.108     -1.247      0.212      -0.346       0.077\n",
       "63             0.1208      0.047      2.578      0.010       0.029       0.213\n",
       "64            -0.1411      0.078     -1.813      0.070      -0.294       0.012\n",
       "65             0.0208      0.033      0.632      0.527      -0.044       0.085\n",
       "66             0.0366      0.061      0.601      0.548      -0.083       0.156\n",
       "67            -0.0795      0.077     -1.033      0.302      -0.230       0.071\n",
       "68             0.0313      0.059      0.527      0.598      -0.085       0.148\n",
       "70            -0.2509      0.159     -1.580      0.114      -0.562       0.061\n",
       "71             0.4107      0.272      1.508      0.132      -0.123       0.945\n",
       "72            -0.1473      0.215     -0.684      0.494      -0.570       0.275\n",
       "73            -0.0194      0.070     -0.278      0.781      -0.156       0.117\n",
       "74            -0.1274      0.068     -1.868      0.062      -0.261       0.006\n",
       "76            -0.1316      0.077     -1.711      0.087      -0.282       0.019\n",
       "77             0.5507      0.212      2.599      0.009       0.135       0.966\n",
       "78            -0.0413      0.167     -0.248      0.804      -0.368       0.285\n",
       "79            -0.2434      0.079     -3.064      0.002      -0.399      -0.088\n",
       "80            -0.7197      0.355     -2.028      0.043      -1.416      -0.024\n",
       "81             0.2077      0.074      2.794      0.005       0.062       0.354\n",
       "82             0.0852      0.058      1.466      0.143      -0.029       0.199\n",
       "83             0.0286      0.019      1.478      0.140      -0.009       0.067\n",
       "84             0.1519      0.073      2.084      0.037       0.009       0.295\n",
       "85            -0.0499      0.031     -1.627      0.104      -0.110       0.010\n",
       "86             0.5822      0.372      1.567      0.117      -0.147       1.311\n",
       "87             0.0521      0.021      2.450      0.014       0.010       0.094\n",
       "88            -0.0732      0.025     -2.936      0.003      -0.122      -0.024\n",
       "89            -0.0257      0.028     -0.925      0.355      -0.080       0.029\n",
       "90             0.0134      0.035      0.382      0.702      -0.055       0.082\n",
       "91            -0.0147      0.020     -0.729      0.466      -0.054       0.025\n",
       "92            -0.3968      0.201     -1.970      0.049      -0.792      -0.002\n",
       "93             0.2652      0.305      0.871      0.384      -0.332       0.863\n",
       "94             0.0213      0.163      0.131      0.896      -0.298       0.341\n",
       "95            -0.2280      0.058     -3.906      0.000      -0.342      -0.113\n",
       "97            -0.0576      0.082     -0.706      0.480      -0.218       0.102\n",
       "98             0.3427      0.100      3.441      0.001       0.147       0.538\n",
       "99             0.0413      0.032      1.281      0.200      -0.022       0.105\n",
       "100           -0.0388      0.033     -1.175      0.240      -0.104       0.026\n",
       "101           -0.0709      0.024     -2.930      0.003      -0.118      -0.023\n",
       "102            0.1472      0.064      2.301      0.021       0.022       0.273\n",
       "103            0.1743      0.047      3.703      0.000       0.082       0.267\n",
       "104            0.1062      0.089      1.199      0.231      -0.067       0.280\n",
       "105            0.0152      0.041      0.371      0.710      -0.065       0.096\n",
       "107            0.0186      0.035      0.533      0.594      -0.050       0.087\n",
       "108            0.0164      0.041      0.400      0.689      -0.064       0.097\n",
       "109            0.0182      0.049      0.372      0.710      -0.078       0.114\n",
       "110           -0.0103      0.030     -0.343      0.732      -0.070       0.049\n",
       "111           -0.0384      0.023     -1.693      0.091      -0.083       0.006\n",
       "112            0.0234      0.015      1.519      0.129      -0.007       0.054\n",
       "==============================================================================\n",
       "Omnibus:                      374.385   Durbin-Watson:                   2.010\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1195.488\n",
       "Skew:                           0.936   Prob(JB):                    2.53e-260\n",
       "Kurtosis:                       6.301   Cond. No.                     2.24e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 4.33e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate 2\n",
    "\n",
    "# Backward elimination by looking at P-values and R-squared and Adjusted R-squared\n",
    "\n",
    "X_opt = pd.DataFrame(X_opt)\n",
    "\n",
    "X_opt = X_opt.drop([14,45,56,59,69,75,96,106],axis = 1)\n",
    "\n",
    "y= list(y)\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   48.31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:55:27</td>     <th>  Log-Likelihood:    </th> <td>  1268.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2354.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1901</td>      <th>  BIC:               </th> <td>  -1839.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    91</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>   <td>    0.5313</td> <td>    0.151</td> <td>    3.514</td> <td> 0.000</td> <td>    0.235</td> <td>    0.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>   <td>    0.0711</td> <td>    0.018</td> <td>    4.032</td> <td> 0.000</td> <td>    0.037</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>   <td>    0.0503</td> <td>    0.018</td> <td>    2.848</td> <td> 0.004</td> <td>    0.016</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>   <td>    0.0576</td> <td>    0.018</td> <td>    3.263</td> <td> 0.001</td> <td>    0.023</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>    0.0427</td> <td>    0.018</td> <td>    2.402</td> <td> 0.016</td> <td>    0.008</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.0582</td> <td>    0.017</td> <td>    3.351</td> <td> 0.001</td> <td>    0.024</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0609</td> <td>    0.018</td> <td>    3.429</td> <td> 0.001</td> <td>    0.026</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.0468</td> <td>    0.018</td> <td>    2.655</td> <td> 0.008</td> <td>    0.012</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0493</td> <td>    0.018</td> <td>    2.802</td> <td> 0.005</td> <td>    0.015</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0536</td> <td>    0.017</td> <td>    3.077</td> <td> 0.002</td> <td>    0.019</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0409</td> <td>    0.018</td> <td>    2.322</td> <td> 0.020</td> <td>    0.006</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>   -0.0007</td> <td>    0.000</td> <td>   -2.816</td> <td> 0.005</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>  <td>-3.545e-06</td> <td> 5.75e-06</td> <td>   -0.616</td> <td> 0.538</td> <td>-1.48e-05</td> <td> 7.74e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>  <td>    0.1915</td> <td>    0.385</td> <td>    0.497</td> <td> 0.619</td> <td>   -0.564</td> <td>    0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>    0.2004</td> <td>    0.050</td> <td>    4.038</td> <td> 0.000</td> <td>    0.103</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>  <td>   -0.0515</td> <td>    0.057</td> <td>   -0.903</td> <td> 0.367</td> <td>   -0.163</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>  <td>   -0.0122</td> <td>    0.032</td> <td>   -0.383</td> <td> 0.702</td> <td>   -0.074</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>    0.0666</td> <td>    0.042</td> <td>    1.593</td> <td> 0.111</td> <td>   -0.015</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>  <td>    0.0982</td> <td>    0.099</td> <td>    0.992</td> <td> 0.321</td> <td>   -0.096</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.2483</td> <td>    0.145</td> <td>   -1.718</td> <td> 0.086</td> <td>   -0.532</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>  <td>   -0.1090</td> <td>    0.155</td> <td>   -0.702</td> <td> 0.483</td> <td>   -0.414</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>  <td>   -0.2863</td> <td>    0.381</td> <td>   -0.751</td> <td> 0.452</td> <td>   -1.033</td> <td>    0.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.0472</td> <td>    0.015</td> <td>    3.109</td> <td> 0.002</td> <td>    0.017</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>  <td>   -0.1705</td> <td>    0.168</td> <td>   -1.014</td> <td> 0.311</td> <td>   -0.500</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.1885</td> <td>    0.081</td> <td>   -2.330</td> <td> 0.020</td> <td>   -0.347</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>    0.0495</td> <td>    0.020</td> <td>    2.538</td> <td> 0.011</td> <td>    0.011</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.1685</td> <td>    0.064</td> <td>   -2.636</td> <td> 0.008</td> <td>   -0.294</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.1222</td> <td>    0.081</td> <td>    1.508</td> <td> 0.132</td> <td>   -0.037</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>   -0.0897</td> <td>    0.035</td> <td>   -2.560</td> <td> 0.011</td> <td>   -0.158</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.2795</td> <td>    0.155</td> <td>    1.807</td> <td> 0.071</td> <td>   -0.024</td> <td>    0.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th>  <td>    0.0687</td> <td>    0.181</td> <td>    0.380</td> <td> 0.704</td> <td>   -0.286</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.3157</td> <td>    0.142</td> <td>   -2.221</td> <td> 0.026</td> <td>   -0.595</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>  <td>   -0.0306</td> <td>    0.025</td> <td>   -1.218</td> <td> 0.223</td> <td>   -0.080</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>   -0.0338</td> <td>    0.019</td> <td>   -1.758</td> <td> 0.079</td> <td>   -0.072</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>  <td>    0.0190</td> <td>    0.019</td> <td>    1.016</td> <td> 0.310</td> <td>   -0.018</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.0454</td> <td>    0.019</td> <td>    2.442</td> <td> 0.015</td> <td>    0.009</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>    0.0340</td> <td>    0.025</td> <td>    1.380</td> <td> 0.168</td> <td>   -0.014</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>  <td>    0.1122</td> <td>    0.135</td> <td>    0.832</td> <td> 0.406</td> <td>   -0.152</td> <td>    0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>   -0.1521</td> <td>    0.056</td> <td>   -2.699</td> <td> 0.007</td> <td>   -0.263</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>   -0.0659</td> <td>    0.038</td> <td>   -1.724</td> <td> 0.085</td> <td>   -0.141</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>44</th>  <td>    0.0264</td> <td>    0.074</td> <td>    0.355</td> <td> 0.723</td> <td>   -0.120</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>    0.2538</td> <td>    0.064</td> <td>    3.970</td> <td> 0.000</td> <td>    0.128</td> <td>    0.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0631</td> <td>    0.032</td> <td>   -1.987</td> <td> 0.047</td> <td>   -0.125</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>48</th>  <td>   -0.0266</td> <td>    0.040</td> <td>   -0.671</td> <td> 0.502</td> <td>   -0.105</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>    0.0793</td> <td>    0.053</td> <td>    1.485</td> <td> 0.138</td> <td>   -0.025</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.1273</td> <td>    0.084</td> <td>    1.517</td> <td> 0.129</td> <td>   -0.037</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.3516</td> <td>    0.097</td> <td>    3.609</td> <td> 0.000</td> <td>    0.161</td> <td>    0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.2207</td> <td>    0.061</td> <td>    3.641</td> <td> 0.000</td> <td>    0.102</td> <td>    0.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.3101</td> <td>    0.107</td> <td>   -2.909</td> <td> 0.004</td> <td>   -0.519</td> <td>   -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>   -0.1434</td> <td>    0.137</td> <td>   -1.045</td> <td> 0.296</td> <td>   -0.413</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.2995</td> <td>    0.088</td> <td>   -3.392</td> <td> 0.001</td> <td>   -0.473</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>58</th>  <td>   -0.0254</td> <td>    0.045</td> <td>   -0.564</td> <td> 0.573</td> <td>   -0.114</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>  <td>    0.0551</td> <td>    0.046</td> <td>    1.186</td> <td> 0.236</td> <td>   -0.036</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.1891</td> <td>    0.053</td> <td>   -3.582</td> <td> 0.000</td> <td>   -0.293</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.1423</td> <td>    0.106</td> <td>   -1.347</td> <td> 0.178</td> <td>   -0.350</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.1186</td> <td>    0.046</td> <td>    2.572</td> <td> 0.010</td> <td>    0.028</td> <td>    0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.1462</td> <td>    0.076</td> <td>   -1.917</td> <td> 0.055</td> <td>   -0.296</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>  <td>    0.0185</td> <td>    0.033</td> <td>    0.567</td> <td> 0.571</td> <td>   -0.046</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>66</th>  <td>    0.0374</td> <td>    0.061</td> <td>    0.617</td> <td> 0.537</td> <td>   -0.081</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>  <td>   -0.0788</td> <td>    0.076</td> <td>   -1.033</td> <td> 0.302</td> <td>   -0.228</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>  <td>    0.0335</td> <td>    0.059</td> <td>    0.570</td> <td> 0.568</td> <td>   -0.082</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>   -0.2561</td> <td>    0.158</td> <td>   -1.626</td> <td> 0.104</td> <td>   -0.565</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>    0.4016</td> <td>    0.271</td> <td>    1.485</td> <td> 0.138</td> <td>   -0.129</td> <td>    0.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>  <td>   -0.1372</td> <td>    0.213</td> <td>   -0.645</td> <td> 0.519</td> <td>   -0.555</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>   -0.1285</td> <td>    0.063</td> <td>   -2.029</td> <td> 0.043</td> <td>   -0.253</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>   -0.1308</td> <td>    0.074</td> <td>   -1.756</td> <td> 0.079</td> <td>   -0.277</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>    0.4982</td> <td>    0.152</td> <td>    3.280</td> <td> 0.001</td> <td>    0.200</td> <td>    0.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>   -0.2440</td> <td>    0.078</td> <td>   -3.110</td> <td> 0.002</td> <td>   -0.398</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.7796</td> <td>    0.228</td> <td>   -3.413</td> <td> 0.001</td> <td>   -1.228</td> <td>   -0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>    0.2092</td> <td>    0.073</td> <td>    2.865</td> <td> 0.004</td> <td>    0.066</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>    0.0795</td> <td>    0.057</td> <td>    1.394</td> <td> 0.163</td> <td>   -0.032</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>    0.0270</td> <td>    0.019</td> <td>    1.412</td> <td> 0.158</td> <td>   -0.011</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>    0.1561</td> <td>    0.071</td> <td>    2.190</td> <td> 0.029</td> <td>    0.016</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>   -0.0525</td> <td>    0.030</td> <td>   -1.773</td> <td> 0.076</td> <td>   -0.111</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>    0.6558</td> <td>    0.229</td> <td>    2.860</td> <td> 0.004</td> <td>    0.206</td> <td>    1.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>    0.0528</td> <td>    0.021</td> <td>    2.500</td> <td> 0.013</td> <td>    0.011</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0728</td> <td>    0.025</td> <td>   -2.955</td> <td> 0.003</td> <td>   -0.121</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>  <td>   -0.0257</td> <td>    0.026</td> <td>   -0.980</td> <td> 0.327</td> <td>   -0.077</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>  <td>   -0.0127</td> <td>    0.020</td> <td>   -0.645</td> <td> 0.519</td> <td>   -0.051</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.4128</td> <td>    0.171</td> <td>   -2.419</td> <td> 0.016</td> <td>   -0.747</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>    0.3080</td> <td>    0.164</td> <td>    1.876</td> <td> 0.061</td> <td>   -0.014</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>   -0.2319</td> <td>    0.057</td> <td>   -4.060</td> <td> 0.000</td> <td>   -0.344</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>   -0.0684</td> <td>    0.081</td> <td>   -0.848</td> <td> 0.397</td> <td>   -0.227</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>    0.3539</td> <td>    0.099</td> <td>    3.589</td> <td> 0.000</td> <td>    0.160</td> <td>    0.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>  <td>    0.0396</td> <td>    0.032</td> <td>    1.248</td> <td> 0.212</td> <td>   -0.023</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0400</td> <td>    0.033</td> <td>   -1.227</td> <td> 0.220</td> <td>   -0.104</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>   -0.0696</td> <td>    0.024</td> <td>   -2.929</td> <td> 0.003</td> <td>   -0.116</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>    0.1469</td> <td>    0.063</td> <td>    2.318</td> <td> 0.021</td> <td>    0.023</td> <td>    0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.1738</td> <td>    0.047</td> <td>    3.720</td> <td> 0.000</td> <td>    0.082</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.1056</td> <td>    0.076</td> <td>    1.388</td> <td> 0.165</td> <td>   -0.044</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th> <td>    0.0332</td> <td>    0.031</td> <td>    1.083</td> <td> 0.279</td> <td>   -0.027</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>   -0.0426</td> <td>    0.022</td> <td>   -1.965</td> <td> 0.050</td> <td>   -0.085</td> <td>-7.63e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.0230</td> <td>    0.015</td> <td>    1.507</td> <td> 0.132</td> <td>   -0.007</td> <td>    0.053</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>375.611</td> <th>  Durbin-Watson:     </th> <td>   2.013</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1203.368</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.938</td>  <th>  Prob(JB):          </th> <td>4.92e-262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.313</td>  <th>  Cond. No.          </th> <td>1.52e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.698\n",
       "Model:                            OLS   Adj. R-squared:                  0.684\n",
       "Method:                 Least Squares   F-statistic:                     48.31\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        13:55:27   Log-Likelihood:                 1268.9\n",
       "No. Observations:                1993   AIC:                            -2354.\n",
       "Df Residuals:                    1901   BIC:                            -1839.\n",
       "Df Model:                          91                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.5313      0.151      3.514      0.000       0.235       0.828\n",
       "1              0.0711      0.018      4.032      0.000       0.037       0.106\n",
       "2              0.0503      0.018      2.848      0.004       0.016       0.085\n",
       "3              0.0576      0.018      3.263      0.001       0.023       0.092\n",
       "4              0.0427      0.018      2.402      0.016       0.008       0.077\n",
       "5              0.0582      0.017      3.351      0.001       0.024       0.092\n",
       "6              0.0609      0.018      3.429      0.001       0.026       0.096\n",
       "7              0.0468      0.018      2.655      0.008       0.012       0.081\n",
       "8              0.0493      0.018      2.802      0.005       0.015       0.084\n",
       "9              0.0536      0.017      3.077      0.002       0.019       0.088\n",
       "10             0.0409      0.018      2.322      0.020       0.006       0.075\n",
       "11            -0.0007      0.000     -2.816      0.005      -0.001      -0.000\n",
       "12         -3.545e-06   5.75e-06     -0.616      0.538   -1.48e-05    7.74e-06\n",
       "13             0.1915      0.385      0.497      0.619      -0.564       0.947\n",
       "15             0.2004      0.050      4.038      0.000       0.103       0.298\n",
       "16            -0.0515      0.057     -0.903      0.367      -0.163       0.060\n",
       "17            -0.0122      0.032     -0.383      0.702      -0.074       0.050\n",
       "18             0.0666      0.042      1.593      0.111      -0.015       0.149\n",
       "19             0.0982      0.099      0.992      0.321      -0.096       0.292\n",
       "20            -0.2483      0.145     -1.718      0.086      -0.532       0.035\n",
       "21            -0.1090      0.155     -0.702      0.483      -0.414       0.196\n",
       "23            -0.2863      0.381     -0.751      0.452      -1.033       0.461\n",
       "24             0.0472      0.015      3.109      0.002       0.017       0.077\n",
       "25            -0.1705      0.168     -1.014      0.311      -0.500       0.159\n",
       "26            -0.1885      0.081     -2.330      0.020      -0.347      -0.030\n",
       "27             0.0495      0.020      2.538      0.011       0.011       0.088\n",
       "28            -0.1685      0.064     -2.636      0.008      -0.294      -0.043\n",
       "29             0.1222      0.081      1.508      0.132      -0.037       0.281\n",
       "31            -0.0897      0.035     -2.560      0.011      -0.158      -0.021\n",
       "32             0.2795      0.155      1.807      0.071      -0.024       0.583\n",
       "33             0.0687      0.181      0.380      0.704      -0.286       0.424\n",
       "34            -0.3157      0.142     -2.221      0.026      -0.595      -0.037\n",
       "35            -0.0306      0.025     -1.218      0.223      -0.080       0.019\n",
       "36            -0.0338      0.019     -1.758      0.079      -0.072       0.004\n",
       "37             0.0190      0.019      1.016      0.310      -0.018       0.056\n",
       "38             0.0454      0.019      2.442      0.015       0.009       0.082\n",
       "39             0.0340      0.025      1.380      0.168      -0.014       0.082\n",
       "40             0.1122      0.135      0.832      0.406      -0.152       0.377\n",
       "41            -0.1521      0.056     -2.699      0.007      -0.263      -0.042\n",
       "42            -0.0659      0.038     -1.724      0.085      -0.141       0.009\n",
       "44             0.0264      0.074      0.355      0.723      -0.120       0.172\n",
       "46             0.2538      0.064      3.970      0.000       0.128       0.379\n",
       "47            -0.0631      0.032     -1.987      0.047      -0.125      -0.001\n",
       "48            -0.0266      0.040     -0.671      0.502      -0.105       0.051\n",
       "49             0.0793      0.053      1.485      0.138      -0.025       0.184\n",
       "50             0.1273      0.084      1.517      0.129      -0.037       0.292\n",
       "51             0.3516      0.097      3.609      0.000       0.161       0.543\n",
       "52             0.2207      0.061      3.641      0.000       0.102       0.340\n",
       "54            -0.3101      0.107     -2.909      0.004      -0.519      -0.101\n",
       "55            -0.1434      0.137     -1.045      0.296      -0.413       0.126\n",
       "57            -0.2995      0.088     -3.392      0.001      -0.473      -0.126\n",
       "58            -0.0254      0.045     -0.564      0.573      -0.114       0.063\n",
       "60             0.0551      0.046      1.186      0.236      -0.036       0.146\n",
       "61            -0.1891      0.053     -3.582      0.000      -0.293      -0.086\n",
       "62            -0.1423      0.106     -1.347      0.178      -0.350       0.065\n",
       "63             0.1186      0.046      2.572      0.010       0.028       0.209\n",
       "64            -0.1462      0.076     -1.917      0.055      -0.296       0.003\n",
       "65             0.0185      0.033      0.567      0.571      -0.046       0.083\n",
       "66             0.0374      0.061      0.617      0.537      -0.081       0.156\n",
       "67            -0.0788      0.076     -1.033      0.302      -0.228       0.071\n",
       "68             0.0335      0.059      0.570      0.568      -0.082       0.149\n",
       "70            -0.2561      0.158     -1.626      0.104      -0.565       0.053\n",
       "71             0.4016      0.271      1.485      0.138      -0.129       0.932\n",
       "72            -0.1372      0.213     -0.645      0.519      -0.555       0.280\n",
       "74            -0.1285      0.063     -2.029      0.043      -0.253      -0.004\n",
       "76            -0.1308      0.074     -1.756      0.079      -0.277       0.015\n",
       "77             0.4982      0.152      3.280      0.001       0.200       0.796\n",
       "79            -0.2440      0.078     -3.110      0.002      -0.398      -0.090\n",
       "80            -0.7796      0.228     -3.413      0.001      -1.228      -0.332\n",
       "81             0.2092      0.073      2.865      0.004       0.066       0.352\n",
       "82             0.0795      0.057      1.394      0.163      -0.032       0.191\n",
       "83             0.0270      0.019      1.412      0.158      -0.011       0.065\n",
       "84             0.1561      0.071      2.190      0.029       0.016       0.296\n",
       "85            -0.0525      0.030     -1.773      0.076      -0.111       0.006\n",
       "86             0.6558      0.229      2.860      0.004       0.206       1.105\n",
       "87             0.0528      0.021      2.500      0.013       0.011       0.094\n",
       "88            -0.0728      0.025     -2.955      0.003      -0.121      -0.025\n",
       "89            -0.0257      0.026     -0.980      0.327      -0.077       0.026\n",
       "91            -0.0127      0.020     -0.645      0.519      -0.051       0.026\n",
       "92            -0.4128      0.171     -2.419      0.016      -0.747      -0.078\n",
       "93             0.3080      0.164      1.876      0.061      -0.014       0.630\n",
       "95            -0.2319      0.057     -4.060      0.000      -0.344      -0.120\n",
       "97            -0.0684      0.081     -0.848      0.397      -0.227       0.090\n",
       "98             0.3539      0.099      3.589      0.000       0.160       0.547\n",
       "99             0.0396      0.032      1.248      0.212      -0.023       0.102\n",
       "100           -0.0400      0.033     -1.227      0.220      -0.104       0.024\n",
       "101           -0.0696      0.024     -2.929      0.003      -0.116      -0.023\n",
       "102            0.1469      0.063      2.318      0.021       0.023       0.271\n",
       "103            0.1738      0.047      3.720      0.000       0.082       0.265\n",
       "104            0.1056      0.076      1.388      0.165      -0.044       0.255\n",
       "107            0.0332      0.031      1.083      0.279      -0.027       0.093\n",
       "111           -0.0426      0.022     -1.965      0.050      -0.085   -7.63e-05\n",
       "112            0.0230      0.015      1.507      0.132      -0.007       0.053\n",
       "==============================================================================\n",
       "Omnibus:                      375.611   Durbin-Watson:                   2.013\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1203.368\n",
       "Skew:                           0.938   Prob(JB):                    4.92e-262\n",
       "Kurtosis:                       6.313   Cond. No.                     1.52e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 9.38e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate 3\n",
    "\n",
    "\n",
    "X_opt = X_opt.drop([110,109,108,105,94,90,78,73,53,43,30,22],axis = 1)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are getting better Adj R- squared at each step.We will continue to have all of p-values less than SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   55.13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:48:49</td>     <th>  Log-Likelihood:    </th> <td>  1267.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2372.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1912</td>      <th>  BIC:               </th> <td>  -1919.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    80</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>   <td>    0.5347</td> <td>    0.142</td> <td>    3.754</td> <td> 0.000</td> <td>    0.255</td> <td>    0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>   <td>    0.0716</td> <td>    0.017</td> <td>    4.246</td> <td> 0.000</td> <td>    0.039</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>   <td>    0.0511</td> <td>    0.017</td> <td>    3.041</td> <td> 0.002</td> <td>    0.018</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>   <td>    0.0582</td> <td>    0.017</td> <td>    3.452</td> <td> 0.001</td> <td>    0.025</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>    0.0429</td> <td>    0.017</td> <td>    2.512</td> <td> 0.012</td> <td>    0.009</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.0581</td> <td>    0.017</td> <td>    3.480</td> <td> 0.001</td> <td>    0.025</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0609</td> <td>    0.017</td> <td>    3.575</td> <td> 0.000</td> <td>    0.028</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.0469</td> <td>    0.017</td> <td>    2.785</td> <td> 0.005</td> <td>    0.014</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0499</td> <td>    0.017</td> <td>    2.973</td> <td> 0.003</td> <td>    0.017</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0535</td> <td>    0.017</td> <td>    3.225</td> <td> 0.001</td> <td>    0.021</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0416</td> <td>    0.017</td> <td>    2.472</td> <td> 0.014</td> <td>    0.009</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>   -0.0007</td> <td>    0.000</td> <td>   -2.908</td> <td> 0.004</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>    0.1993</td> <td>    0.046</td> <td>    4.343</td> <td> 0.000</td> <td>    0.109</td> <td>    0.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>  <td>   -0.0395</td> <td>    0.049</td> <td>   -0.805</td> <td> 0.421</td> <td>   -0.136</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>    0.0718</td> <td>    0.039</td> <td>    1.818</td> <td> 0.069</td> <td>   -0.006</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>  <td>    0.0450</td> <td>    0.062</td> <td>    0.731</td> <td> 0.465</td> <td>   -0.076</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.3268</td> <td>    0.098</td> <td>   -3.321</td> <td> 0.001</td> <td>   -0.520</td> <td>   -0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>  <td>   -0.1057</td> <td>    0.111</td> <td>   -0.949</td> <td> 0.343</td> <td>   -0.324</td> <td>    0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.0414</td> <td>    0.010</td> <td>    4.162</td> <td> 0.000</td> <td>    0.022</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>  <td>   -0.1724</td> <td>    0.165</td> <td>   -1.043</td> <td> 0.297</td> <td>   -0.497</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.1915</td> <td>    0.080</td> <td>   -2.402</td> <td> 0.016</td> <td>   -0.348</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>    0.0478</td> <td>    0.019</td> <td>    2.466</td> <td> 0.014</td> <td>    0.010</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.1688</td> <td>    0.062</td> <td>   -2.715</td> <td> 0.007</td> <td>   -0.291</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.1061</td> <td>    0.078</td> <td>    1.368</td> <td> 0.171</td> <td>   -0.046</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>   -0.0939</td> <td>    0.034</td> <td>   -2.734</td> <td> 0.006</td> <td>   -0.161</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.3155</td> <td>    0.145</td> <td>    2.174</td> <td> 0.030</td> <td>    0.031</td> <td>    0.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.2593</td> <td>    0.077</td> <td>   -3.356</td> <td> 0.001</td> <td>   -0.411</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>  <td>   -0.0300</td> <td>    0.025</td> <td>   -1.209</td> <td> 0.227</td> <td>   -0.079</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>   -0.0337</td> <td>    0.019</td> <td>   -1.761</td> <td> 0.078</td> <td>   -0.071</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>  <td>    0.0187</td> <td>    0.018</td> <td>    1.015</td> <td> 0.310</td> <td>   -0.017</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.0458</td> <td>    0.018</td> <td>    2.484</td> <td> 0.013</td> <td>    0.010</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>    0.0323</td> <td>    0.024</td> <td>    1.325</td> <td> 0.185</td> <td>   -0.016</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>  <td>    0.1056</td> <td>    0.133</td> <td>    0.797</td> <td> 0.426</td> <td>   -0.154</td> <td>    0.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>   -0.1455</td> <td>    0.054</td> <td>   -2.700</td> <td> 0.007</td> <td>   -0.251</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>   -0.0666</td> <td>    0.038</td> <td>   -1.769</td> <td> 0.077</td> <td>   -0.141</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>    0.2577</td> <td>    0.063</td> <td>    4.109</td> <td> 0.000</td> <td>    0.135</td> <td>    0.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0561</td> <td>    0.030</td> <td>   -1.861</td> <td> 0.063</td> <td>   -0.115</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>    0.0768</td> <td>    0.052</td> <td>    1.465</td> <td> 0.143</td> <td>   -0.026</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.1232</td> <td>    0.056</td> <td>    2.198</td> <td> 0.028</td> <td>    0.013</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.3411</td> <td>    0.094</td> <td>    3.614</td> <td> 0.000</td> <td>    0.156</td> <td>    0.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.2077</td> <td>    0.059</td> <td>    3.546</td> <td> 0.000</td> <td>    0.093</td> <td>    0.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.2898</td> <td>    0.104</td> <td>   -2.791</td> <td> 0.005</td> <td>   -0.493</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>   -0.1410</td> <td>    0.123</td> <td>   -1.146</td> <td> 0.252</td> <td>   -0.382</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.3086</td> <td>    0.080</td> <td>   -3.876</td> <td> 0.000</td> <td>   -0.465</td> <td>   -0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>  <td>    0.0541</td> <td>    0.046</td> <td>    1.173</td> <td> 0.241</td> <td>   -0.036</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.1914</td> <td>    0.052</td> <td>   -3.696</td> <td> 0.000</td> <td>   -0.293</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.1387</td> <td>    0.104</td> <td>   -1.332</td> <td> 0.183</td> <td>   -0.343</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.1300</td> <td>    0.044</td> <td>    2.927</td> <td> 0.003</td> <td>    0.043</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.1435</td> <td>    0.076</td> <td>   -1.895</td> <td> 0.058</td> <td>   -0.292</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>  <td>    0.0274</td> <td>    0.027</td> <td>    1.000</td> <td> 0.317</td> <td>   -0.026</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>  <td>   -0.0241</td> <td>    0.034</td> <td>   -0.705</td> <td> 0.481</td> <td>   -0.091</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>   -0.2002</td> <td>    0.138</td> <td>   -1.456</td> <td> 0.146</td> <td>   -0.470</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>    0.2872</td> <td>    0.234</td> <td>    1.227</td> <td> 0.220</td> <td>   -0.172</td> <td>    0.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>  <td>   -0.0716</td> <td>    0.180</td> <td>   -0.398</td> <td> 0.691</td> <td>   -0.424</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>   -0.1266</td> <td>    0.062</td> <td>   -2.038</td> <td> 0.042</td> <td>   -0.248</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>   -0.1157</td> <td>    0.072</td> <td>   -1.610</td> <td> 0.108</td> <td>   -0.257</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>    0.4969</td> <td>    0.148</td> <td>    3.354</td> <td> 0.001</td> <td>    0.206</td> <td>    0.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>   -0.2368</td> <td>    0.076</td> <td>   -3.097</td> <td> 0.002</td> <td>   -0.387</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.7719</td> <td>    0.225</td> <td>   -3.429</td> <td> 0.001</td> <td>   -1.213</td> <td>   -0.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>    0.1904</td> <td>    0.071</td> <td>    2.673</td> <td> 0.008</td> <td>    0.051</td> <td>    0.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>    0.0810</td> <td>    0.056</td> <td>    1.443</td> <td> 0.149</td> <td>   -0.029</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>    0.0272</td> <td>    0.019</td> <td>    1.429</td> <td> 0.153</td> <td>   -0.010</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>    0.1708</td> <td>    0.070</td> <td>    2.448</td> <td> 0.014</td> <td>    0.034</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>   -0.0540</td> <td>    0.029</td> <td>   -1.856</td> <td> 0.064</td> <td>   -0.111</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>    0.6413</td> <td>    0.226</td> <td>    2.838</td> <td> 0.005</td> <td>    0.198</td> <td>    1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>    0.0514</td> <td>    0.021</td> <td>    2.461</td> <td> 0.014</td> <td>    0.010</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0745</td> <td>    0.024</td> <td>   -3.051</td> <td> 0.002</td> <td>   -0.122</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>  <td>   -0.0221</td> <td>    0.025</td> <td>   -0.866</td> <td> 0.387</td> <td>   -0.072</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.4072</td> <td>    0.169</td> <td>   -2.416</td> <td> 0.016</td> <td>   -0.738</td> <td>   -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>    0.2957</td> <td>    0.162</td> <td>    1.824</td> <td> 0.068</td> <td>   -0.022</td> <td>    0.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>   -0.2303</td> <td>    0.056</td> <td>   -4.123</td> <td> 0.000</td> <td>   -0.340</td> <td>   -0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>   -0.0706</td> <td>    0.080</td> <td>   -0.884</td> <td> 0.377</td> <td>   -0.227</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>    0.3518</td> <td>    0.097</td> <td>    3.611</td> <td> 0.000</td> <td>    0.161</td> <td>    0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>  <td>    0.0367</td> <td>    0.031</td> <td>    1.174</td> <td> 0.241</td> <td>   -0.025</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0405</td> <td>    0.032</td> <td>   -1.251</td> <td> 0.211</td> <td>   -0.104</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>   -0.0696</td> <td>    0.024</td> <td>   -2.940</td> <td> 0.003</td> <td>   -0.116</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>    0.1433</td> <td>    0.063</td> <td>    2.278</td> <td> 0.023</td> <td>    0.020</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.1715</td> <td>    0.046</td> <td>    3.695</td> <td> 0.000</td> <td>    0.080</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.0961</td> <td>    0.073</td> <td>    1.312</td> <td> 0.190</td> <td>   -0.048</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th> <td>    0.0319</td> <td>    0.030</td> <td>    1.059</td> <td> 0.290</td> <td>   -0.027</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>   -0.0420</td> <td>    0.021</td> <td>   -1.955</td> <td> 0.051</td> <td>   -0.084</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.0231</td> <td>    0.015</td> <td>    1.517</td> <td> 0.129</td> <td>   -0.007</td> <td>    0.053</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>371.121</td> <th>  Durbin-Watson:     </th> <td>   2.009</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1168.970</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.932</td>  <th>  Prob(JB):          </th> <td>1.45e-254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.256</td>  <th>  Cond. No.          </th> <td>1.48e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.698\n",
       "Model:                            OLS   Adj. R-squared:                  0.685\n",
       "Method:                 Least Squares   F-statistic:                     55.13\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        14:48:49   Log-Likelihood:                 1267.1\n",
       "No. Observations:                1993   AIC:                            -2372.\n",
       "Df Residuals:                    1912   BIC:                            -1919.\n",
       "Df Model:                          80                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.5347      0.142      3.754      0.000       0.255       0.814\n",
       "1              0.0716      0.017      4.246      0.000       0.039       0.105\n",
       "2              0.0511      0.017      3.041      0.002       0.018       0.084\n",
       "3              0.0582      0.017      3.452      0.001       0.025       0.091\n",
       "4              0.0429      0.017      2.512      0.012       0.009       0.076\n",
       "5              0.0581      0.017      3.480      0.001       0.025       0.091\n",
       "6              0.0609      0.017      3.575      0.000       0.028       0.094\n",
       "7              0.0469      0.017      2.785      0.005       0.014       0.080\n",
       "8              0.0499      0.017      2.973      0.003       0.017       0.083\n",
       "9              0.0535      0.017      3.225      0.001       0.021       0.086\n",
       "10             0.0416      0.017      2.472      0.014       0.009       0.075\n",
       "11            -0.0007      0.000     -2.908      0.004      -0.001      -0.000\n",
       "15             0.1993      0.046      4.343      0.000       0.109       0.289\n",
       "16            -0.0395      0.049     -0.805      0.421      -0.136       0.057\n",
       "18             0.0718      0.039      1.818      0.069      -0.006       0.149\n",
       "19             0.0450      0.062      0.731      0.465      -0.076       0.166\n",
       "20            -0.3268      0.098     -3.321      0.001      -0.520      -0.134\n",
       "23            -0.1057      0.111     -0.949      0.343      -0.324       0.113\n",
       "24             0.0414      0.010      4.162      0.000       0.022       0.061\n",
       "25            -0.1724      0.165     -1.043      0.297      -0.497       0.152\n",
       "26            -0.1915      0.080     -2.402      0.016      -0.348      -0.035\n",
       "27             0.0478      0.019      2.466      0.014       0.010       0.086\n",
       "28            -0.1688      0.062     -2.715      0.007      -0.291      -0.047\n",
       "29             0.1061      0.078      1.368      0.171      -0.046       0.258\n",
       "31            -0.0939      0.034     -2.734      0.006      -0.161      -0.027\n",
       "32             0.3155      0.145      2.174      0.030       0.031       0.600\n",
       "34            -0.2593      0.077     -3.356      0.001      -0.411      -0.108\n",
       "35            -0.0300      0.025     -1.209      0.227      -0.079       0.019\n",
       "36            -0.0337      0.019     -1.761      0.078      -0.071       0.004\n",
       "37             0.0187      0.018      1.015      0.310      -0.017       0.055\n",
       "38             0.0458      0.018      2.484      0.013       0.010       0.082\n",
       "39             0.0323      0.024      1.325      0.185      -0.016       0.080\n",
       "40             0.1056      0.133      0.797      0.426      -0.154       0.366\n",
       "41            -0.1455      0.054     -2.700      0.007      -0.251      -0.040\n",
       "42            -0.0666      0.038     -1.769      0.077      -0.141       0.007\n",
       "46             0.2577      0.063      4.109      0.000       0.135       0.381\n",
       "47            -0.0561      0.030     -1.861      0.063      -0.115       0.003\n",
       "49             0.0768      0.052      1.465      0.143      -0.026       0.180\n",
       "50             0.1232      0.056      2.198      0.028       0.013       0.233\n",
       "51             0.3411      0.094      3.614      0.000       0.156       0.526\n",
       "52             0.2077      0.059      3.546      0.000       0.093       0.323\n",
       "54            -0.2898      0.104     -2.791      0.005      -0.493      -0.086\n",
       "55            -0.1410      0.123     -1.146      0.252      -0.382       0.100\n",
       "57            -0.3086      0.080     -3.876      0.000      -0.465      -0.152\n",
       "60             0.0541      0.046      1.173      0.241      -0.036       0.145\n",
       "61            -0.1914      0.052     -3.696      0.000      -0.293      -0.090\n",
       "62            -0.1387      0.104     -1.332      0.183      -0.343       0.065\n",
       "63             0.1300      0.044      2.927      0.003       0.043       0.217\n",
       "64            -0.1435      0.076     -1.895      0.058      -0.292       0.005\n",
       "65             0.0274      0.027      1.000      0.317      -0.026       0.081\n",
       "67            -0.0241      0.034     -0.705      0.481      -0.091       0.043\n",
       "70            -0.2002      0.138     -1.456      0.146      -0.470       0.070\n",
       "71             0.2872      0.234      1.227      0.220      -0.172       0.746\n",
       "72            -0.0716      0.180     -0.398      0.691      -0.424       0.281\n",
       "74            -0.1266      0.062     -2.038      0.042      -0.248      -0.005\n",
       "76            -0.1157      0.072     -1.610      0.108      -0.257       0.025\n",
       "77             0.4969      0.148      3.354      0.001       0.206       0.788\n",
       "79            -0.2368      0.076     -3.097      0.002      -0.387      -0.087\n",
       "80            -0.7719      0.225     -3.429      0.001      -1.213      -0.330\n",
       "81             0.1904      0.071      2.673      0.008       0.051       0.330\n",
       "82             0.0810      0.056      1.443      0.149      -0.029       0.191\n",
       "83             0.0272      0.019      1.429      0.153      -0.010       0.065\n",
       "84             0.1708      0.070      2.448      0.014       0.034       0.308\n",
       "85            -0.0540      0.029     -1.856      0.064      -0.111       0.003\n",
       "86             0.6413      0.226      2.838      0.005       0.198       1.084\n",
       "87             0.0514      0.021      2.461      0.014       0.010       0.092\n",
       "88            -0.0745      0.024     -3.051      0.002      -0.122      -0.027\n",
       "89            -0.0221      0.025     -0.866      0.387      -0.072       0.028\n",
       "92            -0.4072      0.169     -2.416      0.016      -0.738      -0.077\n",
       "93             0.2957      0.162      1.824      0.068      -0.022       0.614\n",
       "95            -0.2303      0.056     -4.123      0.000      -0.340      -0.121\n",
       "97            -0.0706      0.080     -0.884      0.377      -0.227       0.086\n",
       "98             0.3518      0.097      3.611      0.000       0.161       0.543\n",
       "99             0.0367      0.031      1.174      0.241      -0.025       0.098\n",
       "100           -0.0405      0.032     -1.251      0.211      -0.104       0.023\n",
       "101           -0.0696      0.024     -2.940      0.003      -0.116      -0.023\n",
       "102            0.1433      0.063      2.278      0.023       0.020       0.267\n",
       "103            0.1715      0.046      3.695      0.000       0.080       0.263\n",
       "104            0.0961      0.073      1.312      0.190      -0.048       0.240\n",
       "107            0.0319      0.030      1.059      0.290      -0.027       0.091\n",
       "111           -0.0420      0.021     -1.955      0.051      -0.084       0.000\n",
       "112            0.0231      0.015      1.517      0.129      -0.007       0.053\n",
       "==============================================================================\n",
       "Omnibus:                      371.121   Durbin-Watson:                   2.009\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1168.970\n",
       "Skew:                           0.932   Prob(JB):                    1.45e-254\n",
       "Kurtosis:                       6.256   Cond. No.                     1.48e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is  1e-26. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate 4\n",
    "\n",
    "\n",
    "X_opt = X_opt.drop([12,13,17,21,33,44,48,58,66,68,91],axis = 1)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   60.46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:51:26</td>     <th>  Log-Likelihood:    </th> <td>  1265.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2382.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1919</td>      <th>  BIC:               </th> <td>  -1968.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    73</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>   <td>    0.5073</td> <td>    0.134</td> <td>    3.778</td> <td> 0.000</td> <td>    0.244</td> <td>    0.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>   <td>    0.0690</td> <td>    0.016</td> <td>    4.259</td> <td> 0.000</td> <td>    0.037</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>   <td>    0.0483</td> <td>    0.016</td> <td>    3.010</td> <td> 0.003</td> <td>    0.017</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>   <td>    0.0549</td> <td>    0.016</td> <td>    3.386</td> <td> 0.001</td> <td>    0.023</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>    0.0403</td> <td>    0.016</td> <td>    2.469</td> <td> 0.014</td> <td>    0.008</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.0560</td> <td>    0.016</td> <td>    3.506</td> <td> 0.000</td> <td>    0.025</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0581</td> <td>    0.016</td> <td>    3.548</td> <td> 0.000</td> <td>    0.026</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.0444</td> <td>    0.016</td> <td>    2.754</td> <td> 0.006</td> <td>    0.013</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0469</td> <td>    0.016</td> <td>    2.905</td> <td> 0.004</td> <td>    0.015</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0509</td> <td>    0.016</td> <td>    3.207</td> <td> 0.001</td> <td>    0.020</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0385</td> <td>    0.016</td> <td>    2.391</td> <td> 0.017</td> <td>    0.007</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>   -0.0007</td> <td>    0.000</td> <td>   -2.910</td> <td> 0.004</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>    0.2239</td> <td>    0.032</td> <td>    6.998</td> <td> 0.000</td> <td>    0.161</td> <td>    0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>    0.0830</td> <td>    0.038</td> <td>    2.187</td> <td> 0.029</td> <td>    0.009</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.2927</td> <td>    0.075</td> <td>   -3.922</td> <td> 0.000</td> <td>   -0.439</td> <td>   -0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.0364</td> <td>    0.009</td> <td>    3.944</td> <td> 0.000</td> <td>    0.018</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>  <td>   -0.1864</td> <td>    0.165</td> <td>   -1.133</td> <td> 0.257</td> <td>   -0.509</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.2048</td> <td>    0.078</td> <td>   -2.619</td> <td> 0.009</td> <td>   -0.358</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>    0.0481</td> <td>    0.019</td> <td>    2.492</td> <td> 0.013</td> <td>    0.010</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.1609</td> <td>    0.061</td> <td>   -2.622</td> <td> 0.009</td> <td>   -0.281</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.0947</td> <td>    0.077</td> <td>    1.238</td> <td> 0.216</td> <td>   -0.055</td> <td>    0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>   -0.0914</td> <td>    0.034</td> <td>   -2.714</td> <td> 0.007</td> <td>   -0.158</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.3211</td> <td>    0.143</td> <td>    2.249</td> <td> 0.025</td> <td>    0.041</td> <td>    0.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.2540</td> <td>    0.076</td> <td>   -3.354</td> <td> 0.001</td> <td>   -0.402</td> <td>   -0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>  <td>   -0.0288</td> <td>    0.025</td> <td>   -1.169</td> <td> 0.243</td> <td>   -0.077</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>   -0.0338</td> <td>    0.019</td> <td>   -1.772</td> <td> 0.076</td> <td>   -0.071</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.0452</td> <td>    0.018</td> <td>    2.461</td> <td> 0.014</td> <td>    0.009</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>    0.0365</td> <td>    0.024</td> <td>    1.510</td> <td> 0.131</td> <td>   -0.011</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>   -0.1339</td> <td>    0.051</td> <td>   -2.606</td> <td> 0.009</td> <td>   -0.235</td> <td>   -0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>   -0.0712</td> <td>    0.037</td> <td>   -1.908</td> <td> 0.057</td> <td>   -0.144</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>    0.2457</td> <td>    0.062</td> <td>    3.973</td> <td> 0.000</td> <td>    0.124</td> <td>    0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0579</td> <td>    0.030</td> <td>   -1.942</td> <td> 0.052</td> <td>   -0.116</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>    0.0817</td> <td>    0.052</td> <td>    1.571</td> <td> 0.116</td> <td>   -0.020</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.1325</td> <td>    0.055</td> <td>    2.396</td> <td> 0.017</td> <td>    0.024</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.3404</td> <td>    0.094</td> <td>    3.629</td> <td> 0.000</td> <td>    0.156</td> <td>    0.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.2100</td> <td>    0.058</td> <td>    3.594</td> <td> 0.000</td> <td>    0.095</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.2975</td> <td>    0.103</td> <td>   -2.886</td> <td> 0.004</td> <td>   -0.500</td> <td>   -0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>   -0.1346</td> <td>    0.121</td> <td>   -1.108</td> <td> 0.268</td> <td>   -0.373</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.3255</td> <td>    0.078</td> <td>   -4.196</td> <td> 0.000</td> <td>   -0.478</td> <td>   -0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>  <td>    0.0521</td> <td>    0.046</td> <td>    1.136</td> <td> 0.256</td> <td>   -0.038</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.1869</td> <td>    0.051</td> <td>   -3.661</td> <td> 0.000</td> <td>   -0.287</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.1097</td> <td>    0.073</td> <td>   -1.504</td> <td> 0.133</td> <td>   -0.253</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.1266</td> <td>    0.044</td> <td>    2.894</td> <td> 0.004</td> <td>    0.041</td> <td>    0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.1564</td> <td>    0.065</td> <td>   -2.420</td> <td> 0.016</td> <td>   -0.283</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>  <td>    0.0143</td> <td>    0.019</td> <td>    0.733</td> <td> 0.464</td> <td>   -0.024</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>   -0.1837</td> <td>    0.130</td> <td>   -1.410</td> <td> 0.159</td> <td>   -0.439</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>    0.1954</td> <td>    0.144</td> <td>    1.355</td> <td> 0.176</td> <td>   -0.087</td> <td>    0.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>   -0.1317</td> <td>    0.061</td> <td>   -2.152</td> <td> 0.031</td> <td>   -0.252</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>   -0.1275</td> <td>    0.071</td> <td>   -1.791</td> <td> 0.073</td> <td>   -0.267</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>    0.5337</td> <td>    0.145</td> <td>    3.691</td> <td> 0.000</td> <td>    0.250</td> <td>    0.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>   -0.2438</td> <td>    0.076</td> <td>   -3.209</td> <td> 0.001</td> <td>   -0.393</td> <td>   -0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.7647</td> <td>    0.224</td> <td>   -3.410</td> <td> 0.001</td> <td>   -1.204</td> <td>   -0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>    0.1942</td> <td>    0.068</td> <td>    2.851</td> <td> 0.004</td> <td>    0.061</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>    0.0816</td> <td>    0.056</td> <td>    1.458</td> <td> 0.145</td> <td>   -0.028</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>    0.0282</td> <td>    0.019</td> <td>    1.486</td> <td> 0.138</td> <td>   -0.009</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>    0.1551</td> <td>    0.048</td> <td>    3.238</td> <td> 0.001</td> <td>    0.061</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>   -0.0557</td> <td>    0.027</td> <td>   -2.080</td> <td> 0.038</td> <td>   -0.108</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>    0.6369</td> <td>    0.225</td> <td>    2.830</td> <td> 0.005</td> <td>    0.196</td> <td>    1.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>    0.0529</td> <td>    0.021</td> <td>    2.550</td> <td> 0.011</td> <td>    0.012</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0728</td> <td>    0.024</td> <td>   -2.992</td> <td> 0.003</td> <td>   -0.121</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>  <td>   -0.0203</td> <td>    0.025</td> <td>   -0.802</td> <td> 0.423</td> <td>   -0.070</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.4283</td> <td>    0.167</td> <td>   -2.568</td> <td> 0.010</td> <td>   -0.755</td> <td>   -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>    0.3229</td> <td>    0.160</td> <td>    2.017</td> <td> 0.044</td> <td>    0.009</td> <td>    0.637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>   -0.2281</td> <td>    0.055</td> <td>   -4.118</td> <td> 0.000</td> <td>   -0.337</td> <td>   -0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>   -0.0870</td> <td>    0.079</td> <td>   -1.098</td> <td> 0.273</td> <td>   -0.243</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>    0.3666</td> <td>    0.097</td> <td>    3.790</td> <td> 0.000</td> <td>    0.177</td> <td>    0.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>  <td>    0.0367</td> <td>    0.031</td> <td>    1.178</td> <td> 0.239</td> <td>   -0.024</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0445</td> <td>    0.032</td> <td>   -1.383</td> <td> 0.167</td> <td>   -0.108</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>   -0.0721</td> <td>    0.023</td> <td>   -3.074</td> <td> 0.002</td> <td>   -0.118</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>    0.1418</td> <td>    0.062</td> <td>    2.284</td> <td> 0.022</td> <td>    0.020</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.1708</td> <td>    0.046</td> <td>    3.721</td> <td> 0.000</td> <td>    0.081</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.1082</td> <td>    0.066</td> <td>    1.645</td> <td> 0.100</td> <td>   -0.021</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th> <td>    0.0349</td> <td>    0.030</td> <td>    1.167</td> <td> 0.243</td> <td>   -0.024</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>   -0.0426</td> <td>    0.021</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.084</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.0214</td> <td>    0.015</td> <td>    1.422</td> <td> 0.155</td> <td>   -0.008</td> <td>    0.051</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.753</td> <th>  Durbin-Watson:     </th> <td>   2.012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1149.187</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.926</td>  <th>  Prob(JB):          </th> <td>2.87e-250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.226</td>  <th>  Cond. No.          </th> <td>1.43e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.697\n",
       "Model:                            OLS   Adj. R-squared:                  0.685\n",
       "Method:                 Least Squares   F-statistic:                     60.46\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        14:51:26   Log-Likelihood:                 1265.1\n",
       "No. Observations:                1993   AIC:                            -2382.\n",
       "Df Residuals:                    1919   BIC:                            -1968.\n",
       "Df Model:                          73                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.5073      0.134      3.778      0.000       0.244       0.771\n",
       "1              0.0690      0.016      4.259      0.000       0.037       0.101\n",
       "2              0.0483      0.016      3.010      0.003       0.017       0.080\n",
       "3              0.0549      0.016      3.386      0.001       0.023       0.087\n",
       "4              0.0403      0.016      2.469      0.014       0.008       0.072\n",
       "5              0.0560      0.016      3.506      0.000       0.025       0.087\n",
       "6              0.0581      0.016      3.548      0.000       0.026       0.090\n",
       "7              0.0444      0.016      2.754      0.006       0.013       0.076\n",
       "8              0.0469      0.016      2.905      0.004       0.015       0.079\n",
       "9              0.0509      0.016      3.207      0.001       0.020       0.082\n",
       "10             0.0385      0.016      2.391      0.017       0.007       0.070\n",
       "11            -0.0007      0.000     -2.910      0.004      -0.001      -0.000\n",
       "15             0.2239      0.032      6.998      0.000       0.161       0.287\n",
       "18             0.0830      0.038      2.187      0.029       0.009       0.157\n",
       "20            -0.2927      0.075     -3.922      0.000      -0.439      -0.146\n",
       "24             0.0364      0.009      3.944      0.000       0.018       0.055\n",
       "25            -0.1864      0.165     -1.133      0.257      -0.509       0.136\n",
       "26            -0.2048      0.078     -2.619      0.009      -0.358      -0.051\n",
       "27             0.0481      0.019      2.492      0.013       0.010       0.086\n",
       "28            -0.1609      0.061     -2.622      0.009      -0.281      -0.041\n",
       "29             0.0947      0.077      1.238      0.216      -0.055       0.245\n",
       "31            -0.0914      0.034     -2.714      0.007      -0.158      -0.025\n",
       "32             0.3211      0.143      2.249      0.025       0.041       0.601\n",
       "34            -0.2540      0.076     -3.354      0.001      -0.402      -0.105\n",
       "35            -0.0288      0.025     -1.169      0.243      -0.077       0.020\n",
       "36            -0.0338      0.019     -1.772      0.076      -0.071       0.004\n",
       "38             0.0452      0.018      2.461      0.014       0.009       0.081\n",
       "39             0.0365      0.024      1.510      0.131      -0.011       0.084\n",
       "41            -0.1339      0.051     -2.606      0.009      -0.235      -0.033\n",
       "42            -0.0712      0.037     -1.908      0.057      -0.144       0.002\n",
       "46             0.2457      0.062      3.973      0.000       0.124       0.367\n",
       "47            -0.0579      0.030     -1.942      0.052      -0.116       0.001\n",
       "49             0.0817      0.052      1.571      0.116      -0.020       0.184\n",
       "50             0.1325      0.055      2.396      0.017       0.024       0.241\n",
       "51             0.3404      0.094      3.629      0.000       0.156       0.524\n",
       "52             0.2100      0.058      3.594      0.000       0.095       0.325\n",
       "54            -0.2975      0.103     -2.886      0.004      -0.500      -0.095\n",
       "55            -0.1346      0.121     -1.108      0.268      -0.373       0.104\n",
       "57            -0.3255      0.078     -4.196      0.000      -0.478      -0.173\n",
       "60             0.0521      0.046      1.136      0.256      -0.038       0.142\n",
       "61            -0.1869      0.051     -3.661      0.000      -0.287      -0.087\n",
       "62            -0.1097      0.073     -1.504      0.133      -0.253       0.033\n",
       "63             0.1266      0.044      2.894      0.004       0.041       0.212\n",
       "64            -0.1564      0.065     -2.420      0.016      -0.283      -0.030\n",
       "65             0.0143      0.019      0.733      0.464      -0.024       0.052\n",
       "70            -0.1837      0.130     -1.410      0.159      -0.439       0.072\n",
       "71             0.1954      0.144      1.355      0.176      -0.087       0.478\n",
       "74            -0.1317      0.061     -2.152      0.031      -0.252      -0.012\n",
       "76            -0.1275      0.071     -1.791      0.073      -0.267       0.012\n",
       "77             0.5337      0.145      3.691      0.000       0.250       0.817\n",
       "79            -0.2438      0.076     -3.209      0.001      -0.393      -0.095\n",
       "80            -0.7647      0.224     -3.410      0.001      -1.204      -0.325\n",
       "81             0.1942      0.068      2.851      0.004       0.061       0.328\n",
       "82             0.0816      0.056      1.458      0.145      -0.028       0.191\n",
       "83             0.0282      0.019      1.486      0.138      -0.009       0.065\n",
       "84             0.1551      0.048      3.238      0.001       0.061       0.249\n",
       "85            -0.0557      0.027     -2.080      0.038      -0.108      -0.003\n",
       "86             0.6369      0.225      2.830      0.005       0.196       1.078\n",
       "87             0.0529      0.021      2.550      0.011       0.012       0.094\n",
       "88            -0.0728      0.024     -2.992      0.003      -0.121      -0.025\n",
       "89            -0.0203      0.025     -0.802      0.423      -0.070       0.029\n",
       "92            -0.4283      0.167     -2.568      0.010      -0.755      -0.101\n",
       "93             0.3229      0.160      2.017      0.044       0.009       0.637\n",
       "95            -0.2281      0.055     -4.118      0.000      -0.337      -0.119\n",
       "97            -0.0870      0.079     -1.098      0.273      -0.243       0.068\n",
       "98             0.3666      0.097      3.790      0.000       0.177       0.556\n",
       "99             0.0367      0.031      1.178      0.239      -0.024       0.098\n",
       "100           -0.0445      0.032     -1.383      0.167      -0.108       0.019\n",
       "101           -0.0721      0.023     -3.074      0.002      -0.118      -0.026\n",
       "102            0.1418      0.062      2.284      0.022       0.020       0.264\n",
       "103            0.1708      0.046      3.721      0.000       0.081       0.261\n",
       "104            0.1082      0.066      1.645      0.100      -0.021       0.237\n",
       "107            0.0349      0.030      1.167      0.243      -0.024       0.093\n",
       "111           -0.0426      0.021     -1.992      0.047      -0.084      -0.001\n",
       "112            0.0214      0.015      1.422      0.155      -0.008       0.051\n",
       "==============================================================================\n",
       "Omnibus:                      367.753   Durbin-Watson:                   2.012\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1149.187\n",
       "Skew:                           0.926   Prob(JB):                    2.87e-250\n",
       "Kurtosis:                       6.226   Cond. No.                     1.43e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.08e-26. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate 5\n",
    "\n",
    "X_opt = X_opt.drop([16,19,23,37,40,67,72],axis = 1)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   66.63</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:55:00</td>     <th>  Log-Likelihood:    </th> <td>  1260.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2386.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1926</td>      <th>  BIC:               </th> <td>  -2011.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    66</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>   <td>    0.5215</td> <td>    0.131</td> <td>    3.991</td> <td> 0.000</td> <td>    0.265</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>   <td>    0.0699</td> <td>    0.016</td> <td>    4.385</td> <td> 0.000</td> <td>    0.039</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>   <td>    0.0499</td> <td>    0.016</td> <td>    3.169</td> <td> 0.002</td> <td>    0.019</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>   <td>    0.0561</td> <td>    0.016</td> <td>    3.528</td> <td> 0.000</td> <td>    0.025</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>    0.0422</td> <td>    0.016</td> <td>    2.648</td> <td> 0.008</td> <td>    0.011</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.0559</td> <td>    0.016</td> <td>    3.576</td> <td> 0.000</td> <td>    0.025</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0594</td> <td>    0.016</td> <td>    3.703</td> <td> 0.000</td> <td>    0.028</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.0467</td> <td>    0.016</td> <td>    2.936</td> <td> 0.003</td> <td>    0.016</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0491</td> <td>    0.016</td> <td>    3.093</td> <td> 0.002</td> <td>    0.018</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0524</td> <td>    0.016</td> <td>    3.378</td> <td> 0.001</td> <td>    0.022</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0398</td> <td>    0.016</td> <td>    2.520</td> <td> 0.012</td> <td>    0.009</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>   -0.0007</td> <td>    0.000</td> <td>   -3.075</td> <td> 0.002</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>    0.2155</td> <td>    0.031</td> <td>    6.886</td> <td> 0.000</td> <td>    0.154</td> <td>    0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>    0.0789</td> <td>    0.038</td> <td>    2.083</td> <td> 0.037</td> <td>    0.005</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.3067</td> <td>    0.070</td> <td>   -4.355</td> <td> 0.000</td> <td>   -0.445</td> <td>   -0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.0387</td> <td>    0.009</td> <td>    4.231</td> <td> 0.000</td> <td>    0.021</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.2190</td> <td>    0.078</td> <td>   -2.815</td> <td> 0.005</td> <td>   -0.371</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>    0.0446</td> <td>    0.019</td> <td>    2.325</td> <td> 0.020</td> <td>    0.007</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.1691</td> <td>    0.061</td> <td>   -2.790</td> <td> 0.005</td> <td>   -0.288</td> <td>   -0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>  <td>    0.1188</td> <td>    0.075</td> <td>    1.593</td> <td> 0.111</td> <td>   -0.027</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>   -0.0959</td> <td>    0.033</td> <td>   -2.867</td> <td> 0.004</td> <td>   -0.162</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.1796</td> <td>    0.093</td> <td>    1.928</td> <td> 0.054</td> <td>   -0.003</td> <td>    0.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.2648</td> <td>    0.074</td> <td>   -3.572</td> <td> 0.000</td> <td>   -0.410</td> <td>   -0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>   -0.0364</td> <td>    0.019</td> <td>   -1.909</td> <td> 0.056</td> <td>   -0.074</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.0436</td> <td>    0.018</td> <td>    2.378</td> <td> 0.017</td> <td>    0.008</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>  <td>    0.0356</td> <td>    0.024</td> <td>    1.482</td> <td> 0.139</td> <td>   -0.012</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>   -0.1009</td> <td>    0.048</td> <td>   -2.108</td> <td> 0.035</td> <td>   -0.195</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>   -0.0789</td> <td>    0.037</td> <td>   -2.149</td> <td> 0.032</td> <td>   -0.151</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>    0.2363</td> <td>    0.061</td> <td>    3.886</td> <td> 0.000</td> <td>    0.117</td> <td>    0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0641</td> <td>    0.029</td> <td>   -2.179</td> <td> 0.029</td> <td>   -0.122</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>    0.0995</td> <td>    0.051</td> <td>    1.947</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.1214</td> <td>    0.055</td> <td>    2.218</td> <td> 0.027</td> <td>    0.014</td> <td>    0.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.3015</td> <td>    0.092</td> <td>    3.275</td> <td> 0.001</td> <td>    0.121</td> <td>    0.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.2325</td> <td>    0.057</td> <td>    4.093</td> <td> 0.000</td> <td>    0.121</td> <td>    0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.2659</td> <td>    0.102</td> <td>   -2.604</td> <td> 0.009</td> <td>   -0.466</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>  <td>   -0.0971</td> <td>    0.119</td> <td>   -0.813</td> <td> 0.416</td> <td>   -0.331</td> <td>    0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.3201</td> <td>    0.076</td> <td>   -4.235</td> <td> 0.000</td> <td>   -0.468</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.1306</td> <td>    0.028</td> <td>   -4.666</td> <td> 0.000</td> <td>   -0.186</td> <td>   -0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>  <td>   -0.1120</td> <td>    0.073</td> <td>   -1.538</td> <td> 0.124</td> <td>   -0.255</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.1328</td> <td>    0.043</td> <td>    3.055</td> <td> 0.002</td> <td>    0.048</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.1557</td> <td>    0.065</td> <td>   -2.414</td> <td> 0.016</td> <td>   -0.282</td> <td>   -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>  <td>   -0.1457</td> <td>    0.124</td> <td>   -1.173</td> <td> 0.241</td> <td>   -0.389</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>  <td>    0.1746</td> <td>    0.143</td> <td>    1.224</td> <td> 0.221</td> <td>   -0.105</td> <td>    0.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>   -0.1408</td> <td>    0.061</td> <td>   -2.314</td> <td> 0.021</td> <td>   -0.260</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>   -0.1192</td> <td>    0.070</td> <td>   -1.703</td> <td> 0.089</td> <td>   -0.257</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>    0.4457</td> <td>    0.131</td> <td>    3.400</td> <td> 0.001</td> <td>    0.189</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>   -0.2301</td> <td>    0.072</td> <td>   -3.179</td> <td> 0.002</td> <td>   -0.372</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.6787</td> <td>    0.218</td> <td>   -3.115</td> <td> 0.002</td> <td>   -1.106</td> <td>   -0.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>    0.2162</td> <td>    0.067</td> <td>    3.218</td> <td> 0.001</td> <td>    0.084</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>  <td>    0.0721</td> <td>    0.055</td> <td>    1.312</td> <td> 0.190</td> <td>   -0.036</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>  <td>    0.0265</td> <td>    0.019</td> <td>    1.403</td> <td> 0.161</td> <td>   -0.011</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>    0.1596</td> <td>    0.048</td> <td>    3.348</td> <td> 0.001</td> <td>    0.066</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>   -0.0455</td> <td>    0.026</td> <td>   -1.727</td> <td> 0.084</td> <td>   -0.097</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>    0.5524</td> <td>    0.216</td> <td>    2.556</td> <td> 0.011</td> <td>    0.129</td> <td>    0.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>    0.0588</td> <td>    0.021</td> <td>    2.863</td> <td> 0.004</td> <td>    0.019</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0622</td> <td>    0.023</td> <td>   -2.665</td> <td> 0.008</td> <td>   -0.108</td> <td>   -0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.4278</td> <td>    0.164</td> <td>   -2.611</td> <td> 0.009</td> <td>   -0.749</td> <td>   -0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>    0.3364</td> <td>    0.159</td> <td>    2.119</td> <td> 0.034</td> <td>    0.025</td> <td>    0.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>   -0.2245</td> <td>    0.055</td> <td>   -4.066</td> <td> 0.000</td> <td>   -0.333</td> <td>   -0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>  <td>   -0.0930</td> <td>    0.077</td> <td>   -1.202</td> <td> 0.229</td> <td>   -0.245</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>    0.3566</td> <td>    0.096</td> <td>    3.710</td> <td> 0.000</td> <td>    0.168</td> <td>    0.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0527</td> <td>    0.030</td> <td>   -1.752</td> <td> 0.080</td> <td>   -0.112</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>   -0.0683</td> <td>    0.023</td> <td>   -2.937</td> <td> 0.003</td> <td>   -0.114</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>    0.1379</td> <td>    0.062</td> <td>    2.225</td> <td> 0.026</td> <td>    0.016</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.1684</td> <td>    0.046</td> <td>    3.668</td> <td> 0.000</td> <td>    0.078</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th> <td>    0.0985</td> <td>    0.064</td> <td>    1.529</td> <td> 0.126</td> <td>   -0.028</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>   -0.0437</td> <td>    0.021</td> <td>   -2.091</td> <td> 0.037</td> <td>   -0.085</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th> <td>    0.0239</td> <td>    0.015</td> <td>    1.593</td> <td> 0.111</td> <td>   -0.006</td> <td>    0.053</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>366.148</td> <th>  Durbin-Watson:     </th> <td>   2.016</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1129.285</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.927</td>  <th>  Prob(JB):          </th> <td>6.01e-246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.188</td>  <th>  Cond. No.          </th> <td>1.23e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.695\n",
       "Model:                            OLS   Adj. R-squared:                  0.685\n",
       "Method:                 Least Squares   F-statistic:                     66.63\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        14:55:00   Log-Likelihood:                 1260.1\n",
       "No. Observations:                1993   AIC:                            -2386.\n",
       "Df Residuals:                    1926   BIC:                            -2011.\n",
       "Df Model:                          66                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.5215      0.131      3.991      0.000       0.265       0.778\n",
       "1              0.0699      0.016      4.385      0.000       0.039       0.101\n",
       "2              0.0499      0.016      3.169      0.002       0.019       0.081\n",
       "3              0.0561      0.016      3.528      0.000       0.025       0.087\n",
       "4              0.0422      0.016      2.648      0.008       0.011       0.073\n",
       "5              0.0559      0.016      3.576      0.000       0.025       0.087\n",
       "6              0.0594      0.016      3.703      0.000       0.028       0.091\n",
       "7              0.0467      0.016      2.936      0.003       0.016       0.078\n",
       "8              0.0491      0.016      3.093      0.002       0.018       0.080\n",
       "9              0.0524      0.016      3.378      0.001       0.022       0.083\n",
       "10             0.0398      0.016      2.520      0.012       0.009       0.071\n",
       "11            -0.0007      0.000     -3.075      0.002      -0.001      -0.000\n",
       "15             0.2155      0.031      6.886      0.000       0.154       0.277\n",
       "18             0.0789      0.038      2.083      0.037       0.005       0.153\n",
       "20            -0.3067      0.070     -4.355      0.000      -0.445      -0.169\n",
       "24             0.0387      0.009      4.231      0.000       0.021       0.057\n",
       "26            -0.2190      0.078     -2.815      0.005      -0.371      -0.066\n",
       "27             0.0446      0.019      2.325      0.020       0.007       0.082\n",
       "28            -0.1691      0.061     -2.790      0.005      -0.288      -0.050\n",
       "29             0.1188      0.075      1.593      0.111      -0.027       0.265\n",
       "31            -0.0959      0.033     -2.867      0.004      -0.162      -0.030\n",
       "32             0.1796      0.093      1.928      0.054      -0.003       0.362\n",
       "34            -0.2648      0.074     -3.572      0.000      -0.410      -0.119\n",
       "36            -0.0364      0.019     -1.909      0.056      -0.074       0.001\n",
       "38             0.0436      0.018      2.378      0.017       0.008       0.079\n",
       "39             0.0356      0.024      1.482      0.139      -0.012       0.083\n",
       "41            -0.1009      0.048     -2.108      0.035      -0.195      -0.007\n",
       "42            -0.0789      0.037     -2.149      0.032      -0.151      -0.007\n",
       "46             0.2363      0.061      3.886      0.000       0.117       0.356\n",
       "47            -0.0641      0.029     -2.179      0.029      -0.122      -0.006\n",
       "49             0.0995      0.051      1.947      0.052      -0.001       0.200\n",
       "50             0.1214      0.055      2.218      0.027       0.014       0.229\n",
       "51             0.3015      0.092      3.275      0.001       0.121       0.482\n",
       "52             0.2325      0.057      4.093      0.000       0.121       0.344\n",
       "54            -0.2659      0.102     -2.604      0.009      -0.466      -0.066\n",
       "55            -0.0971      0.119     -0.813      0.416      -0.331       0.137\n",
       "57            -0.3201      0.076     -4.235      0.000      -0.468      -0.172\n",
       "61            -0.1306      0.028     -4.666      0.000      -0.186      -0.076\n",
       "62            -0.1120      0.073     -1.538      0.124      -0.255       0.031\n",
       "63             0.1328      0.043      3.055      0.002       0.048       0.218\n",
       "64            -0.1557      0.065     -2.414      0.016      -0.282      -0.029\n",
       "70            -0.1457      0.124     -1.173      0.241      -0.389       0.098\n",
       "71             0.1746      0.143      1.224      0.221      -0.105       0.454\n",
       "74            -0.1408      0.061     -2.314      0.021      -0.260      -0.021\n",
       "76            -0.1192      0.070     -1.703      0.089      -0.257       0.018\n",
       "77             0.4457      0.131      3.400      0.001       0.189       0.703\n",
       "79            -0.2301      0.072     -3.179      0.002      -0.372      -0.088\n",
       "80            -0.6787      0.218     -3.115      0.002      -1.106      -0.251\n",
       "81             0.2162      0.067      3.218      0.001       0.084       0.348\n",
       "82             0.0721      0.055      1.312      0.190      -0.036       0.180\n",
       "83             0.0265      0.019      1.403      0.161      -0.011       0.064\n",
       "84             0.1596      0.048      3.348      0.001       0.066       0.253\n",
       "85            -0.0455      0.026     -1.727      0.084      -0.097       0.006\n",
       "86             0.5524      0.216      2.556      0.011       0.129       0.976\n",
       "87             0.0588      0.021      2.863      0.004       0.019       0.099\n",
       "88            -0.0622      0.023     -2.665      0.008      -0.108      -0.016\n",
       "92            -0.4278      0.164     -2.611      0.009      -0.749      -0.107\n",
       "93             0.3364      0.159      2.119      0.034       0.025       0.648\n",
       "95            -0.2245      0.055     -4.066      0.000      -0.333      -0.116\n",
       "97            -0.0930      0.077     -1.202      0.229      -0.245       0.059\n",
       "98             0.3566      0.096      3.710      0.000       0.168       0.545\n",
       "100           -0.0527      0.030     -1.752      0.080      -0.112       0.006\n",
       "101           -0.0683      0.023     -2.937      0.003      -0.114      -0.023\n",
       "102            0.1379      0.062      2.225      0.026       0.016       0.260\n",
       "103            0.1684      0.046      3.668      0.000       0.078       0.258\n",
       "104            0.0985      0.064      1.529      0.126      -0.028       0.225\n",
       "111           -0.0437      0.021     -2.091      0.037      -0.085      -0.003\n",
       "112            0.0239      0.015      1.593      0.111      -0.006       0.053\n",
       "==============================================================================\n",
       "Omnibus:                      366.148   Durbin-Watson:                   2.016\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1129.285\n",
       "Skew:                           0.927   Prob(JB):                    6.01e-246\n",
       "Kurtosis:                       6.188   Cond. No.                     1.23e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.45e-26. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate 6\n",
    "\n",
    "X_opt = X_opt.drop([107,99,89,65,60,35,25],axis = 1)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   78.82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Apr 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:57:55</td>     <th>  Log-Likelihood:    </th> <td>  1246.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1993</td>      <th>  AIC:               </th> <td>  -2381.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1937</td>      <th>  BIC:               </th> <td>  -2067.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    55</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>   <td>    0.7420</td> <td>    0.094</td> <td>    7.876</td> <td> 0.000</td> <td>    0.557</td> <td>    0.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>   <td>    0.0943</td> <td>    0.013</td> <td>    7.349</td> <td> 0.000</td> <td>    0.069</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>   <td>    0.0737</td> <td>    0.013</td> <td>    5.826</td> <td> 0.000</td> <td>    0.049</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>   <td>    0.0770</td> <td>    0.013</td> <td>    5.913</td> <td> 0.000</td> <td>    0.051</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>   <td>    0.0645</td> <td>    0.013</td> <td>    4.940</td> <td> 0.000</td> <td>    0.039</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>   <td>    0.0786</td> <td>    0.013</td> <td>    6.074</td> <td> 0.000</td> <td>    0.053</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>   <td>    0.0801</td> <td>    0.013</td> <td>    5.969</td> <td> 0.000</td> <td>    0.054</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>   <td>    0.0683</td> <td>    0.013</td> <td>    5.160</td> <td> 0.000</td> <td>    0.042</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>   <td>    0.0712</td> <td>    0.013</td> <td>    5.486</td> <td> 0.000</td> <td>    0.046</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>   <td>    0.0742</td> <td>    0.013</td> <td>    5.761</td> <td> 0.000</td> <td>    0.049</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>  <td>    0.0599</td> <td>    0.013</td> <td>    4.665</td> <td> 0.000</td> <td>    0.035</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>  <td>   -0.0008</td> <td>    0.000</td> <td>   -3.531</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>  <td>    0.2259</td> <td>    0.031</td> <td>    7.396</td> <td> 0.000</td> <td>    0.166</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>  <td>    0.0492</td> <td>    0.036</td> <td>    1.351</td> <td> 0.177</td> <td>   -0.022</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>  <td>   -0.3029</td> <td>    0.069</td> <td>   -4.377</td> <td> 0.000</td> <td>   -0.439</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>  <td>    0.0443</td> <td>    0.009</td> <td>    4.951</td> <td> 0.000</td> <td>    0.027</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>  <td>   -0.2693</td> <td>    0.061</td> <td>   -4.428</td> <td> 0.000</td> <td>   -0.389</td> <td>   -0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>  <td>    0.0390</td> <td>    0.019</td> <td>    2.040</td> <td> 0.042</td> <td>    0.002</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>  <td>   -0.1681</td> <td>    0.060</td> <td>   -2.792</td> <td> 0.005</td> <td>   -0.286</td> <td>   -0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>  <td>   -0.0983</td> <td>    0.032</td> <td>   -3.072</td> <td> 0.002</td> <td>   -0.161</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>  <td>    0.1552</td> <td>    0.092</td> <td>    1.685</td> <td> 0.092</td> <td>   -0.025</td> <td>    0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>  <td>   -0.2387</td> <td>    0.073</td> <td>   -3.268</td> <td> 0.001</td> <td>   -0.382</td> <td>   -0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>  <td>   -0.0381</td> <td>    0.019</td> <td>   -1.999</td> <td> 0.046</td> <td>   -0.076</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>  <td>    0.0554</td> <td>    0.017</td> <td>    3.173</td> <td> 0.002</td> <td>    0.021</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>  <td>   -0.1441</td> <td>    0.046</td> <td>   -3.149</td> <td> 0.002</td> <td>   -0.234</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>  <td>   -0.0736</td> <td>    0.034</td> <td>   -2.143</td> <td> 0.032</td> <td>   -0.141</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>  <td>    0.1782</td> <td>    0.058</td> <td>    3.093</td> <td> 0.002</td> <td>    0.065</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>  <td>   -0.0596</td> <td>    0.029</td> <td>   -2.026</td> <td> 0.043</td> <td>   -0.117</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>  <td>    0.0988</td> <td>    0.051</td> <td>    1.937</td> <td> 0.053</td> <td>   -0.001</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>  <td>    0.1015</td> <td>    0.053</td> <td>    1.897</td> <td> 0.058</td> <td>   -0.003</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>  <td>    0.3100</td> <td>    0.092</td> <td>    3.384</td> <td> 0.001</td> <td>    0.130</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>  <td>    0.2063</td> <td>    0.055</td> <td>    3.746</td> <td> 0.000</td> <td>    0.098</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>  <td>   -0.3087</td> <td>    0.101</td> <td>   -3.053</td> <td> 0.002</td> <td>   -0.507</td> <td>   -0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>  <td>   -0.3487</td> <td>    0.074</td> <td>   -4.721</td> <td> 0.000</td> <td>   -0.494</td> <td>   -0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>  <td>   -0.1232</td> <td>    0.027</td> <td>   -4.531</td> <td> 0.000</td> <td>   -0.177</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>  <td>    0.1078</td> <td>    0.042</td> <td>    2.570</td> <td> 0.010</td> <td>    0.026</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>  <td>   -0.1394</td> <td>    0.062</td> <td>   -2.239</td> <td> 0.025</td> <td>   -0.262</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>  <td>   -0.0329</td> <td>    0.047</td> <td>   -0.706</td> <td> 0.481</td> <td>   -0.124</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>  <td>   -0.1181</td> <td>    0.059</td> <td>   -2.004</td> <td> 0.045</td> <td>   -0.234</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>  <td>    0.2619</td> <td>    0.091</td> <td>    2.880</td> <td> 0.004</td> <td>    0.084</td> <td>    0.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>  <td>   -0.2112</td> <td>    0.070</td> <td>   -3.020</td> <td> 0.003</td> <td>   -0.348</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>  <td>   -0.6644</td> <td>    0.209</td> <td>   -3.184</td> <td> 0.001</td> <td>   -1.074</td> <td>   -0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>  <td>    0.2657</td> <td>    0.063</td> <td>    4.221</td> <td> 0.000</td> <td>    0.142</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>  <td>    0.1321</td> <td>    0.038</td> <td>    3.518</td> <td> 0.000</td> <td>    0.058</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>  <td>   -0.0499</td> <td>    0.025</td> <td>   -2.000</td> <td> 0.046</td> <td>   -0.099</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>  <td>    0.5488</td> <td>    0.205</td> <td>    2.674</td> <td> 0.008</td> <td>    0.146</td> <td>    0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>  <td>    0.0491</td> <td>    0.020</td> <td>    2.416</td> <td> 0.016</td> <td>    0.009</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>  <td>   -0.0595</td> <td>    0.023</td> <td>   -2.571</td> <td> 0.010</td> <td>   -0.105</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>  <td>   -0.3634</td> <td>    0.162</td> <td>   -2.242</td> <td> 0.025</td> <td>   -0.681</td> <td>   -0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>  <td>    0.2890</td> <td>    0.158</td> <td>    1.834</td> <td> 0.067</td> <td>   -0.020</td> <td>    0.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>  <td>   -0.1858</td> <td>    0.054</td> <td>   -3.441</td> <td> 0.001</td> <td>   -0.292</td> <td>   -0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>  <td>    0.2594</td> <td>    0.069</td> <td>    3.754</td> <td> 0.000</td> <td>    0.124</td> <td>    0.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th> <td>   -0.0498</td> <td>    0.029</td> <td>   -1.738</td> <td> 0.082</td> <td>   -0.106</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th> <td>   -0.0628</td> <td>    0.022</td> <td>   -2.812</td> <td> 0.005</td> <td>   -0.107</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th> <td>    0.0630</td> <td>    0.058</td> <td>    1.087</td> <td> 0.277</td> <td>   -0.051</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th> <td>    0.1686</td> <td>    0.046</td> <td>    3.665</td> <td> 0.000</td> <td>    0.078</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th> <td>   -0.0324</td> <td>    0.020</td> <td>   -1.601</td> <td> 0.110</td> <td>   -0.072</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>366.915</td> <th>  Durbin-Watson:     </th> <td>   2.019</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1116.340</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.933</td>  <th>  Prob(JB):          </th> <td>3.89e-243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.156</td>  <th>  Cond. No.          </th> <td>1.41e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.691\n",
       "Model:                            OLS   Adj. R-squared:                  0.682\n",
       "Method:                 Least Squares   F-statistic:                     78.82\n",
       "Date:                Wed, 11 Apr 2018   Prob (F-statistic):               0.00\n",
       "Time:                        14:57:55   Log-Likelihood:                 1246.3\n",
       "No. Observations:                1993   AIC:                            -2381.\n",
       "Df Residuals:                    1937   BIC:                            -2067.\n",
       "Df Model:                          55                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.7420      0.094      7.876      0.000       0.557       0.927\n",
       "1              0.0943      0.013      7.349      0.000       0.069       0.119\n",
       "2              0.0737      0.013      5.826      0.000       0.049       0.099\n",
       "3              0.0770      0.013      5.913      0.000       0.051       0.103\n",
       "4              0.0645      0.013      4.940      0.000       0.039       0.090\n",
       "5              0.0786      0.013      6.074      0.000       0.053       0.104\n",
       "6              0.0801      0.013      5.969      0.000       0.054       0.106\n",
       "7              0.0683      0.013      5.160      0.000       0.042       0.094\n",
       "8              0.0712      0.013      5.486      0.000       0.046       0.097\n",
       "9              0.0742      0.013      5.761      0.000       0.049       0.100\n",
       "10             0.0599      0.013      4.665      0.000       0.035       0.085\n",
       "11            -0.0008      0.000     -3.531      0.000      -0.001      -0.000\n",
       "15             0.2259      0.031      7.396      0.000       0.166       0.286\n",
       "18             0.0492      0.036      1.351      0.177      -0.022       0.121\n",
       "20            -0.3029      0.069     -4.377      0.000      -0.439      -0.167\n",
       "24             0.0443      0.009      4.951      0.000       0.027       0.062\n",
       "26            -0.2693      0.061     -4.428      0.000      -0.389      -0.150\n",
       "27             0.0390      0.019      2.040      0.042       0.002       0.076\n",
       "28            -0.1681      0.060     -2.792      0.005      -0.286      -0.050\n",
       "31            -0.0983      0.032     -3.072      0.002      -0.161      -0.036\n",
       "32             0.1552      0.092      1.685      0.092      -0.025       0.336\n",
       "34            -0.2387      0.073     -3.268      0.001      -0.382      -0.095\n",
       "36            -0.0381      0.019     -1.999      0.046      -0.076      -0.001\n",
       "38             0.0554      0.017      3.173      0.002       0.021       0.090\n",
       "41            -0.1441      0.046     -3.149      0.002      -0.234      -0.054\n",
       "42            -0.0736      0.034     -2.143      0.032      -0.141      -0.006\n",
       "46             0.1782      0.058      3.093      0.002       0.065       0.291\n",
       "47            -0.0596      0.029     -2.026      0.043      -0.117      -0.002\n",
       "49             0.0988      0.051      1.937      0.053      -0.001       0.199\n",
       "50             0.1015      0.053      1.897      0.058      -0.003       0.206\n",
       "51             0.3100      0.092      3.384      0.001       0.130       0.490\n",
       "52             0.2063      0.055      3.746      0.000       0.098       0.314\n",
       "54            -0.3087      0.101     -3.053      0.002      -0.507      -0.110\n",
       "57            -0.3487      0.074     -4.721      0.000      -0.494      -0.204\n",
       "61            -0.1232      0.027     -4.531      0.000      -0.177      -0.070\n",
       "63             0.1078      0.042      2.570      0.010       0.026       0.190\n",
       "64            -0.1394      0.062     -2.239      0.025      -0.262      -0.017\n",
       "74            -0.0329      0.047     -0.706      0.481      -0.124       0.059\n",
       "76            -0.1181      0.059     -2.004      0.045      -0.234      -0.003\n",
       "77             0.2619      0.091      2.880      0.004       0.084       0.440\n",
       "79            -0.2112      0.070     -3.020      0.003      -0.348      -0.074\n",
       "80            -0.6644      0.209     -3.184      0.001      -1.074      -0.255\n",
       "81             0.2657      0.063      4.221      0.000       0.142       0.389\n",
       "84             0.1321      0.038      3.518      0.000       0.058       0.206\n",
       "85            -0.0499      0.025     -2.000      0.046      -0.099      -0.001\n",
       "86             0.5488      0.205      2.674      0.008       0.146       0.951\n",
       "87             0.0491      0.020      2.416      0.016       0.009       0.089\n",
       "88            -0.0595      0.023     -2.571      0.010      -0.105      -0.014\n",
       "92            -0.3634      0.162     -2.242      0.025      -0.681      -0.046\n",
       "93             0.2890      0.158      1.834      0.067      -0.020       0.598\n",
       "95            -0.1858      0.054     -3.441      0.001      -0.292      -0.080\n",
       "98             0.2594      0.069      3.754      0.000       0.124       0.395\n",
       "100           -0.0498      0.029     -1.738      0.082      -0.106       0.006\n",
       "101           -0.0628      0.022     -2.812      0.005      -0.107      -0.019\n",
       "102            0.0630      0.058      1.087      0.277      -0.051       0.177\n",
       "103            0.1686      0.046      3.665      0.000       0.078       0.259\n",
       "111           -0.0324      0.020     -1.601      0.110      -0.072       0.007\n",
       "==============================================================================\n",
       "Omnibus:                      366.915   Durbin-Watson:                   2.019\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1116.340\n",
       "Skew:                           0.933   Prob(JB):                    3.89e-243\n",
       "Kurtosis:                       6.156   Cond. No.                     1.41e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.1e-26. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate 7\n",
    "\n",
    "X_opt = X_opt.drop([29,39,55,62,70,71,82,83,97,104,112],axis = 1)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the iteration number 7 we see our Adj R-squared is decreased. So the model from 6th itration would have better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression algorithms to predict the price of houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption:\n",
    "I take 'v_cat_1' column as a price for houses and in this column there are some Null values which are replaced by the Median price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "crim = pd.read_csv('crime_prep.csv')\n",
    "y = crim['v_cat_1'].fillna(value = crim['v_cat_1'].median())\n",
    "crim.dropna(inplace=True,axis = 1)\n",
    "X = crim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# Ther are two of them\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_cat2 = LabelEncoder()\n",
    "X.iloc[:, 2] = labelencoder_cat2.fit_transform(X.iloc[:, 2])\n",
    "labelencoder_cat3 = LabelEncoder()\n",
    "X.iloc[:, 3] = labelencoder_cat3.fit_transform(X.iloc[:,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to generate Dummy variable for these two variables\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting  Linear Regression to the Training set\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting the Test set results\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.334555234141\n",
      "/n\n",
      "0.0691896722985\n"
     ]
    }
   ],
   "source": [
    "#K-flod cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = regressor, X = X_train,y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print('/n')\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Polinomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Linear Regression to the dataset\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fitting Polynomial Regression to the dataset\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg = PolynomialFeatures(degree = 2)\n",
    "X_poly = poly_reg.fit_transform(X_train)\n",
    "poly_reg.fit(X_poly, y_train)\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(X_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.334555234141\n",
      "/n\n",
      "0.0691896722985\n"
     ]
    }
   ],
   "source": [
    "#K-flod cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = lin_reg_2, X = X_train,y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print('/n')\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decition Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Decision Tree Regression to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "Dtree = DecisionTreeRegressor(random_state = 0)\n",
    "Dtree.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting the Test set results\n",
    "\n",
    "y_pred = Dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470341015963\n",
      "/n\n",
      "0.0954936406229\n"
     ]
    }
   ],
   "source": [
    "#K-flod cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = Dtree, X = X_train,y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print('/n')\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Random Forest Regression to the dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 300, random_state = 0)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "#Predicting the Test set results\n",
    "y_pred = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767206865677\n",
      "/n\n",
      "0.0515616695011\n"
     ]
    }
   ],
   "source": [
    "#K-flod cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = rfr, X = X_train,y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print('/n')\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR - Supper Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting SVR to the dataset\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Predicting the Test set results\n",
    "y_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00800212577512\n",
      "/n\n",
      "0.00714910119759\n"
     ]
    }
   ],
   "source": [
    "#K-flod cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = svr, X = X_train,y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print('/n')\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the ANN \n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu', input_dim = 112))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 20, nb_epoch = 200)\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of ANN is too weak!!. I've executed this model in Spyder and by changing lots of hyper parametrs, such as activation functions ,number of neurons and number of layers I did not get any good result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the random forest has the best perfomance among of Linear Regression,Decistion tree,Polinomial Regression,SVM Regression and ANN.\n",
    "But in these models we can still try to find as better as possible models by doing Grid search for every model.We just need to make a dictionary of hyper parameters that we want to try them, and as I explained in Exercise 1(in Support Vector Machines-SVM section)  we need just to use GridSearchCV class and passe the the dictionary as an input.We need to passe every regressor of the model as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
